{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab A.I. for SEA Challenge: Traffic Management\n",
    "\n",
    "Jayson Yodico <br>\n",
    "Asian Institute of Management\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Understanding congestion dynamics is indispensable in solving traffic congestion.\n",
    "\n",
    "This project seeks to accurately forecast travel demand on a specific area and time based on historical Grab bookings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:38:13.492990Z",
     "start_time": "2019-06-16T03:38:13.472801Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import pygeohash as gh\n",
    "\n",
    "import scipy\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "Upon inspection, it was observed that the are no demand data for some location-time bucket pairs. It is necessary to fill up missing values to make all geohash time series complete.\n",
    "\n",
    "Since it was assumed that all demand data were included in the the timeframe of this training dataset, geohash-time bucket pairs with no demand data are assumed to be zero. This simply means that there is zero demand in that location and time bucket.\n",
    "\n",
    "The purpose of the preprocessing is to fill up gaps in the time series and to make sure that values are arranged chronologically for each location. In addition, decoding of the geohashes  to coordinates (latitude, longitude) and included as additional fields in the processed dataset. The steps are outlined as follows:\n",
    "\n",
    "*Note: The training dataset,* `training.csv`*, must be placed in the folder* `Traffic Management` *of the repository. The dataset can be downloaded from:* https://s3-ap-southeast-1.amazonaws.com/grab-aiforsea-dataset/traffic-management.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:38:32.045477Z",
     "start_time": "2019-06-16T03:38:32.040515Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('Traffic Management/training.csv')\n",
    "raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'], format= '%H:%M').dt.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create dummy datetime values to serve as reference in ordering the `day` and `timestamp` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:38:36.205921Z",
     "start_time": "2019-06-16T03:38:36.200963Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = pd.DataFrame()\n",
    "dates['dummy_date'] = pd.date_range(start=pd.datetime(2019, 1, 1),\n",
    "                              periods=len(raw_df.day.unique())+1)\n",
    "dates['day'] = np.arange(1, dates.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With the dummy datetime values, the sequence can be ordered. For each location, `T_n` serves as an ID indicating the order a value appears in that location-time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:50:53.810357Z",
     "start_time": "2019-06-15T14:50:53.480741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>T_n</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:15:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:30:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:45:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp  T_n  day\n",
       "0  00:00:00    0    1\n",
       "1  00:15:00    1    1\n",
       "2  00:30:00    2    1\n",
       "3  00:45:00    3    1\n",
       "4  01:00:00    4    1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a reference table of chronological ordering indices (T_n) mapped to\n",
    "timestamp and day values.\n",
    "\"\"\"\n",
    "\n",
    "timenum = pd.DataFrame(pd.date_range(start=dates.dummy_date.min(),\n",
    "                                     end=dates.dummy_date.max(),\n",
    "                                     freq='15min'), columns=['dummy_datetime'])\n",
    "\n",
    "timenum['dummy_date'] = pd.to_datetime(timenum['dummy_datetime'].dt.date)\n",
    "timenum['timestamp'] = timenum['dummy_datetime'].dt.time\n",
    "\n",
    "timenum['T_n'] = np.arange(timenum.shape[0])\n",
    "timenum = timenum.merge(dates, on='dummy_date', how='left')\n",
    "\n",
    "del timenum['dummy_datetime'], timenum['dummy_date']\n",
    "timenum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:51:26.890173Z",
     "start_time": "2019-06-15T14:51:24.148389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>day</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>demand</th>\n",
       "      <th>T_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp03wc</td>\n",
       "      <td>18</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>1712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp03pn</td>\n",
       "      <td>10</td>\n",
       "      <td>14:30:00</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp09sw</td>\n",
       "      <td>9</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp0991</td>\n",
       "      <td>32</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>0.088755</td>\n",
       "      <td>2996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp090q</td>\n",
       "      <td>15</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6  day timestamp    demand   T_n\n",
       "0   qp03wc   18  20:00:00  0.020072  1712\n",
       "1   qp03pn   10  14:30:00  0.024721   922\n",
       "2   qp09sw    9  06:15:00  0.102821   793\n",
       "3   qp0991   32  05:00:00  0.088755  2996\n",
       "4   qp090q   15  04:00:00  0.074468  1360"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merging the table with the original dataset\n",
    "\"\"\"\n",
    "df = raw_df.merge(timenum, on=['day', 'timestamp'], how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Filling in gaps in the time series with zeroes (i.e. no demand at that time and location) and decoding the geohashes into latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:52:00.661186Z",
     "start_time": "2019-06-15T14:51:35.096756Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fill gaps in each geohash time series with zeroes\n",
    "\"\"\"\n",
    "g = df.groupby(['geohash6'])\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for loc in g.groups.keys():\n",
    "    \n",
    "    test = g.get_group(loc)\n",
    "\n",
    "    dummy = timenum[(timenum.T_n >= test.T_n.min())\n",
    "                    & (timenum.T_n <= test.T_n.max())]\n",
    "    \n",
    "    # Fill gaps with zeroes\n",
    "    dummy = dummy.merge(test[['T_n', 'demand']], on='T_n', how='left').fillna(0)\n",
    "    dummy['geohash6'] = loc\n",
    "    dummy['lat'] = gh.decode(loc)[0]\n",
    "    dummy['long'] = gh.decode(loc)[1]\n",
    "    \n",
    "    all_data.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:52:08.503983Z",
     "start_time": "2019-06-15T14:52:03.131135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>T_n</th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>geohash6</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02:45:00</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03:15:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03:30:00</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03:45:00</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp  T_n  day    demand geohash6   lat  long\n",
       "0  02:45:00   11    1  0.020592   qp02yc -5.48  90.7\n",
       "1  03:00:00   12    1  0.010292   qp02yc -5.48  90.7\n",
       "2  03:15:00   13    1  0.000000   qp02yc -5.48  90.7\n",
       "3  03:30:00   14    1  0.000000   qp02yc -5.48  90.7\n",
       "4  03:45:00   15    1  0.000000   qp02yc -5.48  90.7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat(all_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Saving the processed file. This will be used to create models in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:54:25.689353Z",
     "start_time": "2019-06-15T14:52:25.874947Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('Traffic Management/training_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process ensures that timestamps for every geohash is put together to create a geohash-time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Development\n",
    "\n",
    "The model development process is centered around three processes:\n",
    "- **Denoising.** Applying a certain level of denoising using Fourier transform\n",
    "- **Windowing**. Given historical values ($T_{n},T_{n-1},.., T_{n-k}$), predict future values ($T_{n+1},T_{n+2}.., T_{n+5}$)\n",
    "- **Genetic Algorithm**. There are many possible model configurations if the data is to be feature-engineered in terms of denoising and windowing. The conventional approach is to use grid search or randomized search in finding the best denoising-windowing configuration. To make the search process more efficient and systematic, this project proposes to use genetic algorithm to find the best denoising-windowing configuration.\n",
    "- **Model Fitting and Performance.** Each window-percentile combination is fitted on a 3-layer neural network model. \n",
    "\n",
    "\n",
    "Model is fitted on a training dataset and updated using a \n",
    "\n",
    "The details of the techniques used in this study are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising using Fourier Transform\n",
    "\n",
    "The first step is to denoise each geohash time series using Fourier transform. Used in signal processing, it postulates that any waveform whose value varies with time can be decomposed into simple waveforms of varying frequencies. From the time domain, the series is projected into the frequency domain though a spectrogram. Take for example a geohash time series:\n",
    "\n",
    "<img src=\"sample_series.png\" width=\"1500\" />\n",
    "\n",
    "in the time domain projected to the frequency domain. The image below represents the distribution of intensities of the frequency components that make up the time series.\n",
    "\n",
    "<img src=\"projected series.png\" width=\"700\" />\n",
    "\n",
    "Fourier transform works as a denoising method by silencing the components with low intensities. From this a threshold could in terms of a percentile value $q$, such that intensities lower than $q$ is set to zero. Higher levels of $q$ means a higher level of denoising. For example, when intensities below the 90th percentile are silenced, the resulting spectrogram is below:\n",
    "\n",
    "<img src=\"projected series silenced.png\" width=\"700\" />\n",
    "\n",
    "This silenced spectrogram, when brought back to the time domain, becomes the following series.\n",
    "\n",
    "<img src=\"cleaned_series.png\" width=\"1500\" />\n",
    "\n",
    "Notice how the denoised series is approximately the shape of the original series, but is less fluctuating. However, too much denoising could ignore the explainable fluctuations of a time series, such that when fitted to a machine learning model, could make less powerful predictions.\n",
    "\n",
    "In this sense, a parameter to be tuned in this process is a *percentile value* denoting level of denoising. The function that executes this process is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(series, pctile):\n",
    "\n",
    "    \"\"\"\n",
    "    Denoises a time series by projecting the series to the frequency domain and\n",
    "    silencing frequencies less than the frequency threshold.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    series: ndarray\n",
    "        - series to denoised\n",
    "\n",
    "    pctile: int\n",
    "        - percentile threshold of denoising. In the frequency domain, all\n",
    "          frequency components with intensities less than the threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    sff = scipy.fft(series)    \n",
    "    abs_sff = abs(sff)\n",
    "    sff[abs_sff < np.percentile(abs_sff, q=pctile)] = 0\n",
    "    cleaned_series = np.abs(scipy.ifft(sff))\n",
    "\n",
    "    return cleaned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "\n",
    "The denoised series will be fitted to a machine learning model. In this project, the machine learning to be used as a 3 -layer neural network model. In principle, given a window value there are a total of window plus 2 features (plus latitude and longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-16T14:58:12.968Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_trainval_test(data, pctile, max_window, frac=0.001):\n",
    "\n",
    "    i = 0\n",
    "    g = data.groupby('geohash6')\n",
    "    \n",
    "    for each_gh in g.groups.keys():\n",
    "        dummy = g.get_group(each_gh).copy()\n",
    "        dummy['demand_fft'] = denoise(dummy.demand.values, pctile)\n",
    "\n",
    "        for fwd in range(-2,-6,-1):\n",
    "            dummy[f'demand+{-1*fwd}'] = dummy['demand'].shift(fwd+1)\n",
    "            dummy[f'demand_fft+{-1*fwd}'] = dummy['demand_fft'].shift(fwd+1)\n",
    "\n",
    "        for bwd in range(1, 6):\n",
    "            dummy[f'demand-{bwd}'] = dummy['demand'].shift(bwd)\n",
    "\n",
    "        for bwd in range(1, max_window + 1):\n",
    "            dummy[f'demand_fft-{bwd}'] = dummy['demand_fft'].shift(bwd)\n",
    "\n",
    "        dummy = dummy.dropna()\n",
    "        len_dummy = dummy.shape[0]\n",
    "        sp1, sp2 = int(len_dummy*0.6), int(len_dummy*0.8)\n",
    "\n",
    "        trn = dummy.iloc[:sp1,:].sample(frac=frac)\n",
    "        vl = dummy.iloc[sp1:sp2,:].sample(frac=frac)\n",
    "        tst = dummy.iloc[sp2:,:].sample(frac=frac)\n",
    "\n",
    "        if i == 0:\n",
    "            trn.to_csv('train1.csv', index=False)\n",
    "            vl.to_csv('val1.csv', index=False)\n",
    "            tst.to_csv('test1.csv', index=False)\n",
    "\n",
    "        else:\n",
    "            trn.to_csv('train1.csv', mode='a', header=False, index=False)\n",
    "            vl.to_csv('val1.csv', mode='a', header=False, index=False)\n",
    "            tst.to_csv('test1.csv', mode='a', header=False, index=False)  \n",
    "        \n",
    "        i+=1\n",
    "            \n",
    "    train = pd.read_csv('train1.csv')\n",
    "    val = pd.read_csv('val1.csv')\n",
    "    test = pd.read_csv('test1.csv')\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def split(train, val, test):\n",
    "\n",
    "    toremove_feat = ['demand','demand+2', 'demand+3','demand+4', 'demand+5',\n",
    "                     'demand_fft','demand_fft+2', 'demand_fft+3','demand_fft+4',\n",
    "                     'demand_fft+5','demand-1', 'demand-2', 'demand-3',\n",
    "                     'demand-4','demand-5', 'geohash6']\n",
    "\n",
    "    targets = ['demand_fft', 'demand_fft+2', 'demand_fft+3', 'demand_fft+4',\n",
    "               'demand_fft+5']\n",
    "    \n",
    "    tags = ['geohash6']\n",
    "\n",
    "    targets_actual = ['demand', 'demand+2', 'demand+3', 'demand+4', 'demand+5']\n",
    "\n",
    "    X_train = train.drop(toremove_feat, axis=1)\n",
    "    y_train = train[targets]\n",
    "    train_tag = train[tags]\n",
    "\n",
    "    X_val = val.drop(toremove_feat, axis=1)\n",
    "    y_val = val[targets]\n",
    "    val_tag = val[tags]\n",
    "\n",
    "    X_test = test.drop(toremove_feat, axis=1)\n",
    "    y_test = test[targets_actual]\n",
    "    test_tag = test[tags]\n",
    "    \n",
    "    return (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "            train_tag, val_tag, test_tag)\n",
    "\n",
    "\n",
    "def fit_model(filepath, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    n_rows, n_cols = X_train.shape\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_cols, input_dim = n_cols, kernel_initializer='uniform',\n",
    "                    activation='linear'))\n",
    "    model.add(Dense(n_cols*5, kernel_initializer='uniform',\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(5, kernel_initializer='uniform', activation='linear'))\n",
    "    \n",
    "    lrs = [0.001, 0.0001]\n",
    "    patience_vals = [20, 200]\n",
    "    epoch_vals = [100, 1000]\n",
    "    \n",
    "    for ind in range(len(lrs)):\n",
    "    \n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=optimizers.RMSprop(lr=lrs[ind]),\n",
    "                      metrics=['mean_squared_error'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath, monitor='val_mean_squared_error',\n",
    "                                 verbose=0, save_best_only=True,\n",
    "                                 mode='min')\n",
    "\n",
    "        es = EarlyStopping(monitor='val_mean_squared_error',\n",
    "                           patience=patience_vals[ind], verbose=0,\n",
    "                           min_delta=0.00001)\n",
    "\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                  epochs=epoch_vals[ind], batch_size=1024*32,\n",
    "                  callbacks=[es, mc],\n",
    "                  verbose=0)\n",
    "\n",
    "        model = load_model(filepath)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_indiv_testmse(model, X_test, y_test):\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    a1 = mean_squared_error(preds[:,0].ravel(), y_test['demand'].values)\n",
    "    a2 = mean_squared_error(preds[:,1].ravel(), y_test['demand+2'].values)\n",
    "    a3 = mean_squared_error(preds[:,2].ravel(), y_test['demand+3'].values)\n",
    "    a4 = mean_squared_error(preds[:,3].ravel(), y_test['demand+4'].values)\n",
    "    a5 = mean_squared_error(preds[:,4].ravel(), y_test['demand+5'].values)\n",
    "\n",
    "    return a1, a2, a3, a4, a5\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    a1 = mean_squared_error(preds[:,0].ravel(), y_test['demand'].values)\n",
    "    a2 = mean_squared_error(preds[:,1].ravel(), y_test['demand+2'].values)\n",
    "    a3 = mean_squared_error(preds[:,2].ravel(), y_test['demand+3'].values)\n",
    "    a4 = mean_squared_error(preds[:,3].ravel(), y_test['demand+4'].values)\n",
    "    a5 = mean_squared_error(preds[:,4].ravel(), y_test['demand+5'].values)\n",
    "\n",
    "    fitness = np.mean([a1, a2, a3, a4, a5])\n",
    "    \n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENETIC ALGORITHM FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T08:06:26.423628Z",
     "start_time": "2019-06-16T08:06:26.042670Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_testmse(data, pctile, window):\n",
    "    \n",
    "    train, val, test = create_trainval_test(data=data, pctile=pctile,\n",
    "                                            max_window= window)\n",
    "    \n",
    "    print('Creating Data: Done', end='\\t')\n",
    "    \n",
    "    (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "     train_tag, val_tag, test_tag) = split(train, val, test)\n",
    "    \n",
    "    model = fit_model('model.hdf5', X_train, y_train,\n",
    "                      X_val, y_val)\n",
    "    \n",
    "    print('Model Fitting: Done', end='\\t')\n",
    "    \n",
    "    fitness = evaluate(model, X_test, y_test)\n",
    "    \n",
    "    print(f'Fitness = {fitness}')\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "def mate(fittest, mating_pairs, n_children):\n",
    "    \n",
    "    species = np.zeros((n_species, len_gene), dtype=int)\n",
    "    species[0] = fittest\n",
    "\n",
    "    i = 1   \n",
    "    for father, mother in mating_pairs: \n",
    "        \n",
    "        parents = [father, mother]\n",
    "        np.random.shuffle(parents)\n",
    "        \n",
    "        child = np.zeros(len_gene, dtype=int)\n",
    "        \n",
    "        child[:1] = parents[0][:1]\n",
    "        child[1:] = parents[1][1:]\n",
    "\n",
    "        species[i] = child\n",
    "        i += 1\n",
    "        if i == n_children: break\n",
    "\n",
    "    return species\n",
    "\n",
    "def mutate(species, mutation_rate):\n",
    "    \n",
    "    n_species = species.shape[0]\n",
    "    n_mutate = int(mutation_rate*(n_species - 1))\n",
    "\n",
    "    inds_mut = np.random.randint(1, n_species, size=n_mutate)\n",
    "    species[inds_mut, 0] = np.random.choice(np.arange(min_pctile, max_pctile, 5), n_mutate)\n",
    "\n",
    "    inds_mut = np.random.randint(1, n_species, size=n_mutate)\n",
    "    species[inds_mut, 1] = np.random.choice(np.arange(min_window, max_window, 4), n_mutate)\n",
    "   \n",
    "    return species\n",
    "\n",
    "def diversify(sp):\n",
    "    \n",
    "    col_to_change = np.random.choice(list(range(len_gene)))\n",
    "    \n",
    "    if col_to_change == 0:\n",
    "        sp[0] = np.random.choice(np.arange(min_pctile, max_pctile, 5),1)# PERCENTILE\n",
    "    \n",
    "    elif col_to_change == 1:\n",
    "        sp[1] = np.random.choice(np.arange(min_window, max_window, 4), 1)\n",
    "    \n",
    "    return sp\n",
    "\n",
    "def add_to_records(all_records, species, fitness_values):\n",
    "    \n",
    "    res_gen = pd.DataFrame(species, columns=['percentile', 'window'])\n",
    "    res_gen['gen'] = gen\n",
    "    res_gen['index'] = list(range(n_species))\n",
    "    res_gen['fitness_test_mse'] = fitness_values\n",
    "    \n",
    "    all_records = pd.concat([all_records, res_gen])\n",
    "    all_records.to_csv('all_records.csv', index=False)\n",
    "    \n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.197135Z",
     "start_time": "2019-06-16T03:44:54.866214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.020592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.010292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6   lat  long    demand\n",
       "0   qp02yc -5.48  90.7  0.020592\n",
       "1   qp02yc -5.48  90.7  0.010292\n",
       "2   qp02yc -5.48  90.7  0.000000\n",
       "3   qp02yc -5.48  90.7  0.000000\n",
       "4   qp02yc -5.48  90.7  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Traffic Management/training_processed.csv')\n",
    "df = df.sort_values(['geohash6', 'T_n'])\n",
    "\n",
    "del df['timestamp'], df['T_n'], df['day']\n",
    "\n",
    "data = df[['geohash6', 'lat', 'long', 'demand']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.227671Z",
     "start_time": "2019-06-16T03:45:17.202084Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "res_file = 'all_records.csv' # Filename of genetic algorithm records\n",
    "exists = os.path.isfile(res_file)\n",
    "\n",
    "n_species = 10\n",
    "len_gene = 2\n",
    "max_gen = 10\n",
    "mut_rates = np.linspace(0, 0.5, max_gen)\n",
    "n_select = 5\n",
    "\n",
    "min_pctile, max_pctile = 5, 100\n",
    "min_window, max_window = 4, 100\n",
    "\n",
    "col_names = ['percentile', 'window', 'gen', 'index', 'fitness_test_mse']\n",
    "gene_names = ['percentile', 'window']\n",
    "\n",
    "tested_species = np.array([-99,-99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initializes the genetic algorithm. When a run is interrupted due to due to unseen circumstances, the user may continue running the GA from most recent generation using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.350381Z",
     "start_time": "2019-06-16T03:45:17.231000Z"
    }
   },
   "outputs": [],
   "source": [
    "if exists:\n",
    "    results_df = pd.read_csv(res_file)\n",
    "    tested_species = results_df[gene_names].values\n",
    "    recent_results = results_df[results_df['gen'] == max(results_df['gen'])][col_names].reset_index(drop=True)\n",
    "\n",
    "    num_generations = list(range(max(results_df['gen'])+1, max_gen))\n",
    "\n",
    "    mating_pool = recent_results[gene_names].values[:n_select]\n",
    "    mating_pairs = list(itertools.combinations(mating_pool, 2))    \n",
    "    np.random.shuffle(mating_pairs)    \n",
    "    \n",
    "    recent_results = recent_results.sort_values('fitness_test_mse')\n",
    "    fittest_val = recent_results.iloc[0]['fitness_test_mse']\n",
    "    fittest = recent_results.iloc[0][gene_names].values.astype(int)\n",
    "    \n",
    "    species = mate(fittest=fittest, mating_pairs=mating_pairs,\n",
    "               n_children=n_species)\n",
    "    species = mutate(species, mutation_rate=mut_rates[num_generations[0]])\n",
    "    \n",
    "    tested_species = results_df[gene_names].values\n",
    "    \n",
    "else:\n",
    "    \n",
    "    num_generations = list(range(0,max_gen))\n",
    "    results_df = pd.DataFrame(columns=col_names)\n",
    "    \n",
    "    # INITIALIZE VALUES\n",
    "    species = np.zeros((n_species, len_gene), dtype=int)\n",
    "    species[:,0] = np.random.choice(np.arange(min_pctile, max_pctile, 5), n_species)\n",
    "    species[:,1] = np.random.choice(np.arange(min_window, max_window, 4), n_species)    \n",
    "    tested_species = np.array([-99,-99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the genetic algorithm from a predefined number of generations. The output below shows a resumed run from the 8th generation (generation index 7) up to the 10th generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:19:39.297946Z",
     "start_time": "2019-06-16T03:45:17.364877Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation 7:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[30 44] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003377516621380486\n",
      "[75 12] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003433179373105824\n",
      "[85 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0033305781445787884\n",
      "[ 5 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003343668680430529\n",
      "[95 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004323591796865457\n",
      "[75 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004525261417953867\n",
      "[45 32] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0027725289104957983\n",
      "[55 80] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003492590125429041\n",
      "[85  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024145148705954724\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n",
      "generation 8:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[25  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025493469318358325\n",
      "[60 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024812099327977976\n",
      "[90 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025470279941015974\n",
      "[40 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0033379552193915085\n",
      "[85  4] Creating Data: Done\tModel Fitting: Done\tFitness = 0.014006311209603255\n",
      "[35  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004556459956561222\n",
      "[ 5 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.002309556548289862\n",
      "[15 12] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0029374550857402893\n",
      "[15 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0031429835664478767\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n",
      "generation 9:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[55 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.002108363880202165\n",
      "[75 28] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0032650283261267594\n",
      "[ 5 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.001974336284176592\n",
      "[10 88] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024385333057059745\n",
      "[20 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025742614876441547\n",
      "[65  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003055786612043888\n",
      "[35 68] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003112448435805511\n",
      "[5 4] Creating Data: Done\tModel Fitting: Done\tFitness = 0.015855070100878832\n",
      "[60 72] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0034283913681592663\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for gen in num_generations:\n",
    "    \n",
    "    print(f'generation {gen}:')\n",
    "    fitness_values = []\n",
    "    \n",
    "    for j, sp in enumerate(species):\n",
    "\n",
    "        if gen == 0:\n",
    "            print(sp, end=' ')\n",
    "            fitness = calculate_testmse(data, sp[0], sp[1])   \n",
    "            tested_species = np.vstack([tested_species, sp])\n",
    "\n",
    "        elif gen > 0:\n",
    "            if j == 0:\n",
    "                print(sp, 'Done', end='\\t')\n",
    "                fitness = fittest_val\n",
    "                print(f'Fitness = {fitness}')\n",
    "\n",
    "            elif j > 0:\n",
    "                while sp.tolist() in tested_species.tolist():\n",
    "                    sp = diversify(sp)\n",
    "                print(sp, end=' ')\n",
    "                species[j,:] = sp\n",
    "                fitness = calculate_testmse(data, sp[0], sp[1])    \n",
    "                tested_species = np.vstack([tested_species, sp])\n",
    "\n",
    "        fitness_values.append(fitness)\n",
    "\n",
    "    res_gen = pd.DataFrame(species, columns=gene_names)\n",
    "    res_gen['gen'] = gen\n",
    "    res_gen['index'] = list(range(n_species))\n",
    "    res_gen['fitness_test_mse'] = fitness_values\n",
    "    \n",
    "    results_df = pd.concat([results_df, res_gen])\n",
    "    results_df.to_csv(res_file, index=False)\n",
    "\n",
    "    fittest, fittest_val = (species[np.argmin(fitness_values)],\n",
    "                            np.min(fitness_values))\n",
    "    \n",
    "    print(f'Fittest:{fittest} {fittest_val}')\n",
    "    print('==================================')\n",
    "    \n",
    "    species = species[np.argsort(fitness_values)]\n",
    "    mating_pool = species[:n_select]\n",
    "    mating_pairs = list(itertools.combinations(mating_pool, 2))    \n",
    "    np.random.shuffle(mating_pairs)\n",
    "    \n",
    "    species = mate(fittest=fittest, mating_pairs=mating_pairs,\n",
    "                   n_children=n_species)\n",
    "    \n",
    "    species = mutate(species, mutation_rate=mut_rates[gen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:01:36.408066Z",
     "start_time": "2019-06-16T15:01:09.386318Z"
    }
   },
   "outputs": [],
   "source": [
    "g = data.groupby('geohash6')\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for each_gh in g.groups.keys():\n",
    "    dummy = g.get_group(each_gh).copy()\n",
    "    \n",
    "    for bwd in range(1, 6):\n",
    "        dummy[f'demand-{bwd}'] = dummy['demand'].shift(bwd)\n",
    "        \n",
    "    dummy = dummy.dropna()\n",
    "    \n",
    "    all_dfs.append(dummy)\n",
    "    \n",
    "data2 = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:01:47.496407Z",
     "start_time": "2019-06-16T15:01:46.391815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0006979268923042981,\n",
       " 0.0010812851416353638,\n",
       " 0.001417649086463842,\n",
       " 0.0017633287859278506,\n",
       " 0.0021435275081951254)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASELINE MSE OF T + 1\n",
    "b1 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-1'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 2\n",
    "b2 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-2'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 3\n",
    "b3 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-3'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 4\n",
    "b4 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-4'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 5\n",
    "b5 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-5'].values)\n",
    "\n",
    "b1, b2, b3, b4, b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:01:50.636732Z",
     "start_time": "2019-06-16T15:01:50.628846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001420743482905296"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([b1, b2, b3, b4, b5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST MODEL CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:01:57.667294Z",
     "start_time": "2019-06-16T15:01:57.612924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentile          45.000000\n",
       "window              92.000000\n",
       "gen                  1.000000\n",
       "index                9.000000\n",
       "fitness_test_mse     0.001964\n",
       "Name: 19, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all = pd.read_csv('all_records.csv')\n",
    "top = (results_all.sort_values(['fitness_test_mse', 'window'])\n",
    "       .drop_duplicates(['percentile', 'window'])).iloc[0]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T14:51:26.326905Z",
     "start_time": "2019-06-16T14:51:26.272062Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 94)                8930      \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 470)               44650     \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 5)                 2355      \n",
      "=================================================================\n",
      "Total params: 55,935\n",
      "Trainable params: 55,935\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best1 = top[['percentile', 'window']].values.astype(int).tolist()\n",
    "\n",
    "train, val, test = create_trainval_test(data=data, pctile=best1[0],\n",
    "                                        max_window=best1[1], frac=0.5)\n",
    "\n",
    "(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    " train_tag, val_tag, test_tag) = split(train, val, test)\n",
    "\n",
    "model = fit_model('model1.hdf5', X_train, y_train,\n",
    "                  X_val, y_val)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:06:49.899949Z",
     "start_time": "2019-06-16T15:06:49.891503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MSE OF T+1 = 0.0006202053193087017\n",
      "TEST MSE OF T+2 = 0.0008698083768624374\n",
      "TEST MSE OF T+3 = 0.0010511852036437458\n",
      "TEST MSE OF T+4 = 0.001205552648903535\n",
      "TEST MSE OF T+5 = 0.0013260415782185345\n"
     ]
    }
   ],
   "source": [
    "mse1, mse2, mse3, mse4, mse5 = evaluate_indiv_testmse(model, X_test, y_test)\n",
    "\n",
    "print(f'TEST MSE OF T+1 = {mse1}')\n",
    "print(f'TEST MSE OF T+2 = {mse2}')\n",
    "print(f'TEST MSE OF T+3 = {mse3}')\n",
    "print(f'TEST MSE OF T+4 = {mse4}')\n",
    "print(f'TEST MSE OF T+5 = {mse5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:08:14.676904Z",
     "start_time": "2019-06-16T15:07:16.511985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL TEST MSE OF MODEL 1 = 0.0010145586253873908\n"
     ]
    }
   ],
   "source": [
    "test_mse = evaluate(model, X_test, y_test)\n",
    "\n",
    "print(f'OVERALL TEST MSE OF MODEL = {test_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T15:09:13.271965Z",
     "start_time": "2019-06-16T15:08:14.682070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL TEST MSE OF MODEL = 0.0010145586253873908\n"
     ]
    }
   ],
   "source": [
    "test_mse = evaluate(model, X_test, y_test)\n",
    "\n",
    "print(f'OVERALL TEST MSE OF MODEL = {test_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT MO NA YUNG HOLDOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE VERBOSITY OF NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_holdout():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_for holdout():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
