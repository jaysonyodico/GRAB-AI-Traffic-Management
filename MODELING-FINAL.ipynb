{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab A.I. for SEA Challenge: Traffic Management\n",
    "\n",
    "Jayson Yodico <br>\n",
    "Asian Institute of Management\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Understanding congestion dynamics is indispensable in solving traffic congestion.\n",
    "\n",
    "This project seeks to accurately forecast travel demand on a specific area and time based on historical Grab bookings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:38:13.492990Z",
     "start_time": "2019-06-16T03:38:13.472801Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import pygeohash as gh\n",
    "\n",
    "import scipy\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "Upon inspection, it was observed that the are no demand data for some location-time bucket pairs. It is necessary to fill up missing values to make all geohash time series complete.\n",
    "\n",
    "Since it was assumed that all demand data were included in the the timeframe of this training dataset, geohash-time bucket pairs with no demand data are assumed to be zero. This simply means that there is zero demand in that location and time bucket.\n",
    "\n",
    "The purpose of the preprocessing is to fill up gaps in the time series and to make sure that values are arranged chronologically for each location. In addition, decoding of the geohashes  to coordinates (latitude, longitude) and included as additional fields in the processed dataset. The steps are outlined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:38:32.045477Z",
     "start_time": "2019-06-16T03:38:32.040515Z"
    }
   },
   "outputs": [],
   "source": [
    "# raw_df = pd.read_csv('Traffic Management/training.csv')\n",
    "# raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'], format= '%H:%M').dt.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create dummy datetime values to serve as reference in ordering the `day` and `timestamp` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:38:36.205921Z",
     "start_time": "2019-06-16T03:38:36.200963Z"
    }
   },
   "outputs": [],
   "source": [
    "# dates = pd.DataFrame()\n",
    "# dates['dummy_date'] = pd.date_range(start=pd.datetime(2019, 1, 1),\n",
    "#                               periods=len(raw_df.day.unique())+1)\n",
    "# dates['day'] = np.arange(1, dates.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With the dummy datetime values, the sequence can be ordered. For each location, `T_n` serves as an ID indicating the order a value appears in that location-time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:50:53.810357Z",
     "start_time": "2019-06-15T14:50:53.480741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>T_n</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:15:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:30:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:45:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp  T_n  day\n",
       "0  00:00:00    0    1\n",
       "1  00:15:00    1    1\n",
       "2  00:30:00    2    1\n",
       "3  00:45:00    3    1\n",
       "4  01:00:00    4    1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenum = pd.DataFrame(pd.date_range(start=dates.dummy_date.min(),\n",
    "                                     end=dates.dummy_date.max(),\n",
    "                                     freq='15min'), columns=['dummy_datetime'])\n",
    "\n",
    "timenum['dummy_date'] = pd.to_datetime(timenum['dummy_datetime'].dt.date)\n",
    "timenum['timestamp'] = timenum['dummy_datetime'].dt.time\n",
    "\n",
    "timenum['T_n'] = np.arange(timenum.shape[0])\n",
    "timenum = timenum.merge(dates, on='dummy_date', how='left')\n",
    "\n",
    "del timenum['dummy_datetime'], timenum['dummy_date']\n",
    "timenum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:51:26.890173Z",
     "start_time": "2019-06-15T14:51:24.148389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>day</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>demand</th>\n",
       "      <th>T_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp03wc</td>\n",
       "      <td>18</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>1712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp03pn</td>\n",
       "      <td>10</td>\n",
       "      <td>14:30:00</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp09sw</td>\n",
       "      <td>9</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp0991</td>\n",
       "      <td>32</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>0.088755</td>\n",
       "      <td>2996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp090q</td>\n",
       "      <td>15</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6  day timestamp    demand   T_n\n",
       "0   qp03wc   18  20:00:00  0.020072  1712\n",
       "1   qp03pn   10  14:30:00  0.024721   922\n",
       "2   qp09sw    9  06:15:00  0.102821   793\n",
       "3   qp0991   32  05:00:00  0.088755  2996\n",
       "4   qp090q   15  04:00:00  0.074468  1360"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SET AS DATETIME INDICES\n",
    "df = raw_df.merge(timenum, on=['day', 'timestamp'], how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Filling in gaps in the time series with zeroes (i.e. no demand at that time and location) and decoding the geohashes into latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:52:00.661186Z",
     "start_time": "2019-06-15T14:51:35.096756Z"
    }
   },
   "outputs": [],
   "source": [
    "g = df.groupby(['geohash6'])\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for loc in g.groups.keys():\n",
    "    \n",
    "    test = g.get_group(loc)\n",
    "\n",
    "    dummy = timenum[(timenum.T_n >= test.T_n.min())\n",
    "                    & (timenum.T_n <= test.T_n.max())]\n",
    "    dummy = dummy.merge(test[['T_n', 'demand']], on='T_n', how='left').fillna(0)\n",
    "    dummy['geohash6'] = loc\n",
    "    dummy['lat'] = gh.decode(loc)[0]\n",
    "    dummy['long'] = gh.decode(loc)[1]\n",
    "    \n",
    "    all_data.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:52:08.503983Z",
     "start_time": "2019-06-15T14:52:03.131135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>T_n</th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>geohash6</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02:45:00</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03:15:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03:30:00</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03:45:00</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp  T_n  day    demand geohash6   lat  long\n",
       "0  02:45:00   11    1  0.020592   qp02yc -5.48  90.7\n",
       "1  03:00:00   12    1  0.010292   qp02yc -5.48  90.7\n",
       "2  03:15:00   13    1  0.000000   qp02yc -5.48  90.7\n",
       "3  03:30:00   14    1  0.000000   qp02yc -5.48  90.7\n",
       "4  03:45:00   15    1  0.000000   qp02yc -5.48  90.7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat(all_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Saving the processed file. This will be used to create models in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:54:25.689353Z",
     "start_time": "2019-06-15T14:52:25.874947Z"
    }
   },
   "outputs": [],
   "source": [
    "# data.to_csv('Traffic Management/training_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING AND MODELING FUNCTIONS\n",
    "\n",
    "The model development process is based on the concept of time series decomposition. \n",
    "Feature engineering centers around two techniques applied in each geohash time series: Fourier transform and windowing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T07:00:07.854350Z",
     "start_time": "2019-06-16T07:00:06.753262Z"
    }
   },
   "outputs": [],
   "source": [
    "def denoise(series, pctile):\n",
    "\n",
    "    sff = scipy.fft(series)    \n",
    "    abs_sff = abs(sff)\n",
    "    sff[abs_sff < np.percentile(abs_sff, q=pctile)] = 0\n",
    "    cleaned_series = np.abs(scipy.ifft(sff))\n",
    "\n",
    "    return cleaned_series\n",
    "\n",
    "def create_trainval_test(data, pctile, max_window, frac=0.001):\n",
    "\n",
    "    i = 0\n",
    "    g = data.groupby('geohash6')\n",
    "    \n",
    "    for each_gh in g.groups.keys():\n",
    "        dummy = g.get_group(each_gh).copy()\n",
    "        dummy['demand_fft'] = denoise(dummy.demand.values, pctile)\n",
    "\n",
    "        for fwd in range(-2,-6,-1):\n",
    "            dummy[f'demand+{-1*fwd}'] = dummy['demand'].shift(fwd+1)\n",
    "            dummy[f'demand_fft+{-1*fwd}'] = dummy['demand_fft'].shift(fwd+1)\n",
    "\n",
    "        for bwd in range(1, 6):\n",
    "            dummy[f'demand-{bwd}'] = dummy['demand'].shift(bwd)\n",
    "\n",
    "        for bwd in range(1, max_window + 1):\n",
    "            dummy[f'demand_fft-{bwd}'] = dummy['demand_fft'].shift(bwd)\n",
    "\n",
    "        dummy = dummy.dropna()\n",
    "        len_dummy = dummy.shape[0]\n",
    "        sp1, sp2 = int(len_dummy*0.6), int(len_dummy*0.8)\n",
    "\n",
    "        trn = dummy.iloc[:sp1,:].sample(frac=frac)\n",
    "        vl = dummy.iloc[sp1:sp2,:].sample(frac=frac)\n",
    "        tst = dummy.iloc[sp2:,:].sample(frac=frac)\n",
    "\n",
    "        if i == 0:\n",
    "            trn.to_csv('train1.csv', index=False)\n",
    "            vl.to_csv('val1.csv', index=False)\n",
    "            tst.to_csv('test1.csv', index=False)\n",
    "\n",
    "        else:\n",
    "            trn.to_csv('train1.csv', mode='a', header=False, index=False)\n",
    "            vl.to_csv('val1.csv', mode='a', header=False, index=False)\n",
    "            tst.to_csv('test1.csv', mode='a', header=False, index=False)  \n",
    "        \n",
    "        i+=1\n",
    "            \n",
    "    train = pd.read_csv('train1.csv')\n",
    "    val = pd.read_csv('val1.csv')\n",
    "    test = pd.read_csv('test1.csv')\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def split(train, val, test):\n",
    "\n",
    "    toremove_feat = ['demand','demand+2', 'demand+3','demand+4', 'demand+5',\n",
    "                     'demand_fft','demand_fft+2', 'demand_fft+3','demand_fft+4',\n",
    "                     'demand_fft+5','demand-1', 'demand-2', 'demand-3',\n",
    "                     'demand-4','demand-5', 'geohash6']\n",
    "\n",
    "    targets = ['demand_fft', 'demand_fft+2', 'demand_fft+3', 'demand_fft+4',\n",
    "               'demand_fft+5']\n",
    "    \n",
    "    tags = ['geohash6']\n",
    "\n",
    "    targets_actual = ['demand', 'demand+2', 'demand+3', 'demand+4', 'demand+5']\n",
    "\n",
    "    X_train = train.drop(toremove_feat, axis=1)\n",
    "    y_train = train[targets]\n",
    "    train_tag = train[tags]\n",
    "\n",
    "    X_val = val.drop(toremove_feat, axis=1)\n",
    "    y_val = val[targets]\n",
    "    val_tag = val[tags]\n",
    "\n",
    "    X_test = test.drop(toremove_feat, axis=1)\n",
    "    y_test = test[targets_actual]\n",
    "    test_tag = test[tags]\n",
    "    \n",
    "    return (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "            train_tag, val_tag, test_tag)\n",
    "\n",
    "\n",
    "def fit_model(filepath, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \n",
    "    n_rows, n_cols = X_train.shape\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_cols, input_dim = n_cols, kernel_initializer='uniform',\n",
    "                    activation='linear'))\n",
    "    model.add(Dense(n_cols*5, kernel_initializer='uniform',\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(5, kernel_initializer='uniform', activation='linear'))\n",
    "    \n",
    "    lrs = [0.001, 0.0001]\n",
    "    patience_vals = [10, 100]\n",
    "    epoch_vals = [50, 500]\n",
    "    \n",
    "    for ind in range(len(lrs)):\n",
    "    \n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=optimizers.RMSprop(lr=lrs[ind]),\n",
    "                      metrics=['mean_squared_error'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath, monitor='val_mean_squared_error',\n",
    "                                 verbose=1, save_best_only=True,\n",
    "                                 mode='min')\n",
    "\n",
    "        es = EarlyStopping(monitor='val_mean_squared_error',\n",
    "                           patience=patience_vals[ind], verbose=1,\n",
    "                           min_delta=0.00001)\n",
    "\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                  epochs=epoch_vals[ind], batch_size=1024*256,\n",
    "                  callbacks=[es, mc],\n",
    "                  verbose=1)\n",
    "\n",
    "        model = load_model(filepath)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    a1 = mean_squared_error(preds[:,0].ravel(), y_test['demand'].values)\n",
    "    a2 = mean_squared_error(preds[:,1].ravel(), y_test['demand+2'].values)\n",
    "    a3 = mean_squared_error(preds[:,2].ravel(), y_test['demand+3'].values)\n",
    "    a4 = mean_squared_error(preds[:,3].ravel(), y_test['demand+4'].values)\n",
    "    a5 = mean_squared_error(preds[:,4].ravel(), y_test['demand+5'].values)\n",
    "\n",
    "    fitness = np.mean([a1, a2, a3, a4, a5])\n",
    "    \n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENETIC ALGORITHM FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:44:50.811005Z",
     "start_time": "2019-06-16T03:44:50.429433Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_testmse(data, pctile, window):\n",
    "    \n",
    "    train, val, test = create_trainval_test(data=data, pctile=pctile,\n",
    "                                            max_window= window)\n",
    "    \n",
    "    print('Creating Data: Done', end='\\t')\n",
    "    \n",
    "    (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "     train_tag, val_tag, test_tag) = split(train, val, test)\n",
    "    \n",
    "    model = fit_model('model.hdf5', X_train, y_train,\n",
    "                      X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    print('Model Fitting: Done', end='\\t')\n",
    "    \n",
    "    fitness = evaluate(model, X_test, y_test)\n",
    "    \n",
    "    print(f'Fitness = {fitness}')\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "def mate(fittest, mating_pairs, n_children):\n",
    "    \n",
    "    species = np.zeros((n_species, len_gene), dtype=int)\n",
    "    species[0] = fittest\n",
    "\n",
    "    i = 1   \n",
    "    for father, mother in mating_pairs: \n",
    "        \n",
    "        parents = [father, mother]\n",
    "        np.random.shuffle(parents)\n",
    "        \n",
    "        child = np.zeros(len_gene, dtype=int)\n",
    "        \n",
    "        child[:1] = parents[0][:1]\n",
    "        child[1:] = parents[1][1:]\n",
    "\n",
    "        species[i] = child\n",
    "        i += 1\n",
    "        if i == n_children: break\n",
    "\n",
    "    return species\n",
    "\n",
    "def mutate(species, mutation_rate):\n",
    "    \n",
    "    n_species = species.shape[0]\n",
    "    n_mutate = int(mutation_rate*(n_species - 1))\n",
    "\n",
    "    inds_mut = np.random.randint(1, n_species, size=n_mutate)\n",
    "    species[inds_mut, 0] = np.random.choice(np.arange(min_pctile, max_pctile, 5), n_mutate)\n",
    "\n",
    "    inds_mut = np.random.randint(1, n_species, size=n_mutate)\n",
    "    species[inds_mut, 1] = np.random.choice(np.arange(min_window, max_window, 4), n_mutate)\n",
    "   \n",
    "    return species\n",
    "\n",
    "def diversify(sp):\n",
    "    \n",
    "    col_to_change = np.random.choice(list(range(len_gene)))\n",
    "    \n",
    "    if col_to_change == 0:\n",
    "        sp[0] = np.random.choice(np.arange(min_pctile, max_pctile, 5),1)# PERCENTILE\n",
    "    \n",
    "    elif col_to_change == 1:\n",
    "        sp[1] = np.random.choice(np.arange(min_window, max_window, 4), 1)\n",
    "    \n",
    "    return sp\n",
    "\n",
    "def add_to_records(all_records, species, fitness_values):\n",
    "    \n",
    "    res_gen = pd.DataFrame(species, columns=['percentile', 'window'])\n",
    "    res_gen['gen'] = gen\n",
    "    res_gen['index'] = list(range(n_species))\n",
    "    res_gen['fitness_test_mse'] = fitness_values\n",
    "    \n",
    "    all_records = pd.concat([all_records, res_gen])\n",
    "    all_records.to_csv('all_records.csv', index=False)\n",
    "    \n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.197135Z",
     "start_time": "2019-06-16T03:44:54.866214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.020592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.010292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6   lat  long    demand\n",
       "0   qp02yc -5.48  90.7  0.020592\n",
       "1   qp02yc -5.48  90.7  0.010292\n",
       "2   qp02yc -5.48  90.7  0.000000\n",
       "3   qp02yc -5.48  90.7  0.000000\n",
       "4   qp02yc -5.48  90.7  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Traffic Management/training_processed.csv')\n",
    "df = df.sort_values(['geohash6', 'T_n'])\n",
    "\n",
    "del df['timestamp'], df['T_n'], df['day']\n",
    "\n",
    "data = df[['geohash6', 'lat', 'long', 'demand']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:06:31.126891Z",
     "start_time": "2019-06-15T14:06:31.119631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7556911, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-15T13:46:48.076Z"
    }
   },
   "outputs": [],
   "source": [
    "# g = data.groupby('geohash6')\n",
    "# all_corrs = []\n",
    "# for pctile in range(10,100, 10):\n",
    "\n",
    "#     all_gh = []\n",
    "    \n",
    "#     for each_gh in g.groups.keys():\n",
    "\n",
    "#         dummy = g.get_group(each_gh).copy()\n",
    "#         dummy['demand_fft'] = denoise(dummy.demand.values, pctile)\n",
    "#         all_gh.append(dummy)\n",
    "        \n",
    "#     all_dfs = pd.concat(all_gh)\n",
    "#     corr = np.corrcoef(all_dfs.demand.values, all_dfs.demand_fft.values)[0,1]\n",
    "#     all_corrs.append(corr)\n",
    "    \n",
    "# fig = plt.figure()\n",
    "# plt.plot(range(10,100, 10), all_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENETIC ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.227671Z",
     "start_time": "2019-06-16T03:45:17.202084Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "res_file = 'all_records.csv'\n",
    "exists = os.path.isfile(res_file)\n",
    "\n",
    "n_species = 10\n",
    "len_gene = 2\n",
    "max_gen = 10\n",
    "mut_rates = np.linspace(0, 0.5, max_gen)\n",
    "n_select = 5\n",
    "\n",
    "min_pctile, max_pctile = 5, 100\n",
    "min_window, max_window = 4, 100\n",
    "\n",
    "col_names = ['percentile', 'window', 'gen', 'index', 'fitness_test_mse']\n",
    "gene_names = ['percentile', 'window']\n",
    "\n",
    "tested_species = np.array([-99,-99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.350381Z",
     "start_time": "2019-06-16T03:45:17.231000Z"
    }
   },
   "outputs": [],
   "source": [
    "if exists:\n",
    "    results_df = pd.read_csv(res_file)\n",
    "    tested_species = results_df[gene_names].values\n",
    "    recent_results = results_df[results_df['gen'] == max(results_df['gen'])][col_names].reset_index(drop=True)\n",
    "\n",
    "    num_generations = list(range(max(results_df['gen'])+1, max_gen))\n",
    "\n",
    "    mating_pool = recent_results[gene_names].values[:n_select]\n",
    "    mating_pairs = list(itertools.combinations(mating_pool, 2))    \n",
    "    np.random.shuffle(mating_pairs)    \n",
    "    \n",
    "    recent_results = recent_results.sort_values('fitness_test_mse')\n",
    "    fittest_val = recent_results.iloc[0]['fitness_test_mse']\n",
    "    fittest = recent_results.iloc[0][gene_names].values.astype(int)\n",
    "    \n",
    "    species = mate(fittest=fittest, mating_pairs=mating_pairs,\n",
    "               n_children=n_species)\n",
    "    species = mutate(species, mutation_rate=mut_rates[num_generations[0]])\n",
    "    \n",
    "    tested_species = results_df[gene_names].values\n",
    "    \n",
    "else:\n",
    "    \n",
    "    num_generations = list(range(0,max_gen))\n",
    "    results_df = pd.DataFrame(columns=col_names)\n",
    "    \n",
    "    # INITIALIZE VALUES\n",
    "    species = np.zeros((n_species, len_gene), dtype=int)\n",
    "    species[:,0] = np.random.choice(np.arange(min_pctile, max_pctile, 5), n_species)\n",
    "    species[:,1] = np.random.choice(np.arange(min_window, max_window, 4), n_species)    \n",
    "    tested_species = np.array([-99,-99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.361041Z",
     "start_time": "2019-06-16T03:45:17.354762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45, 92],\n",
       "       [45, 44],\n",
       "       [75, 12],\n",
       "       [85, 36],\n",
       "       [45, 36],\n",
       "       [95, 52],\n",
       "       [75, 52],\n",
       "       [45, 36],\n",
       "       [45, 88],\n",
       "       [60,  8]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:23:01.091344Z",
     "start_time": "2019-06-16T03:23:01.085217Z"
    }
   },
   "outputs": [],
   "source": [
    "# species[1,:] = np.array([30, 96])\n",
    "\n",
    "# #for j, sp in enumerate(species):\n",
    "# #    if j == 0:\n",
    "# #        continue\n",
    "\n",
    "# #     elif j > 0:\n",
    "# #         while sp.tolist() in tested_species.tolist():\n",
    "# #             sp = diversify(sp)\n",
    "            \n",
    "# species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:19:39.297946Z",
     "start_time": "2019-06-16T03:45:17.364877Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation 7:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[30 44] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003377516621380486\n",
      "[75 12] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003433179373105824\n",
      "[85 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0033305781445787884\n",
      "[ 5 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003343668680430529\n",
      "[95 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004323591796865457\n",
      "[75 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004525261417953867\n",
      "[45 32] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0027725289104957983\n",
      "[55 80] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003492590125429041\n",
      "[85  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024145148705954724\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n",
      "generation 8:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[25  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025493469318358325\n",
      "[60 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024812099327977976\n",
      "[90 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025470279941015974\n",
      "[40 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0033379552193915085\n",
      "[85  4] Creating Data: Done\tModel Fitting: Done\tFitness = 0.014006311209603255\n",
      "[35  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004556459956561222\n",
      "[ 5 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.002309556548289862\n",
      "[15 12] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0029374550857402893\n",
      "[15 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0031429835664478767\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n",
      "generation 9:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[55 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.002108363880202165\n",
      "[75 28] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0032650283261267594\n",
      "[ 5 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.001974336284176592\n",
      "[10 88] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024385333057059745\n",
      "[20 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025742614876441547\n",
      "[65  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003055786612043888\n",
      "[35 68] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003112448435805511\n",
      "[5 4] Creating Data: Done\tModel Fitting: Done\tFitness = 0.015855070100878832\n",
      "[60 72] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0034283913681592663\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for gen in num_generations:\n",
    "    \n",
    "    print(f'generation {gen}:')\n",
    "    fitness_values = []\n",
    "    \n",
    "    for j, sp in enumerate(species):\n",
    "\n",
    "        if gen == 0:\n",
    "            print(sp, end=' ')\n",
    "            fitness = calculate_testmse(data, sp[0], sp[1])   \n",
    "            tested_species = np.vstack([tested_species, sp])\n",
    "\n",
    "        elif gen > 0:\n",
    "            if j == 0:\n",
    "                print(sp, 'Done', end='\\t')\n",
    "                fitness = fittest_val\n",
    "                print(f'Fitness = {fitness}')\n",
    "\n",
    "            elif j > 0:\n",
    "                while sp.tolist() in tested_species.tolist():\n",
    "                    sp = diversify(sp)\n",
    "                print(sp, end=' ')\n",
    "                species[j,:] = sp\n",
    "                fitness = calculate_testmse(data, sp[0], sp[1])    \n",
    "                tested_species = np.vstack([tested_species, sp])\n",
    "\n",
    "        fitness_values.append(fitness)\n",
    "\n",
    "    res_gen = pd.DataFrame(species, columns=gene_names)\n",
    "    res_gen['gen'] = gen\n",
    "    res_gen['index'] = list(range(n_species))\n",
    "    res_gen['fitness_test_mse'] = fitness_values\n",
    "    \n",
    "    results_df = pd.concat([results_df, res_gen])\n",
    "    results_df.to_csv(res_file, index=False)\n",
    "\n",
    "    fittest, fittest_val = (species[np.argmin(fitness_values)],\n",
    "                            np.min(fitness_values))\n",
    "    \n",
    "    print(f'Fittest:{fittest} {fittest_val}')\n",
    "    print('==================================')\n",
    "    \n",
    "    species = species[np.argsort(fitness_values)]\n",
    "    mating_pool = species[:n_select]\n",
    "    mating_pairs = list(itertools.combinations(mating_pool, 2))    \n",
    "    np.random.shuffle(mating_pairs)\n",
    "    \n",
    "    species = mate(fittest=fittest, mating_pairs=mating_pairs,\n",
    "                   n_children=n_species)\n",
    "    \n",
    "    species = mutate(species, mutation_rate=mut_rates[gen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST MODEL CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:41:12.789151Z",
     "start_time": "2019-06-16T06:41:12.747446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentile</th>\n",
       "      <th>window</th>\n",
       "      <th>gen</th>\n",
       "      <th>index</th>\n",
       "      <th>fitness_test_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>45</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>55</td>\n",
       "      <td>92</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    percentile  window  gen  index  fitness_test_mse\n",
       "19          45      92    1      9          0.001964\n",
       "93           5      96    9      3          0.001974\n",
       "9           40      92    0      9          0.001994\n",
       "1           45      96    0      1          0.002042\n",
       "91          55      92    9      1          0.002108"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all = pd.read_csv('all_records.csv')\n",
    "top = (results_all.sort_values(['fitness_test_mse', 'window'])\n",
    "       .drop_duplicates(['percentile', 'window'])).iloc[:5]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:46:52.899360Z",
     "start_time": "2019-06-16T06:46:52.891106Z"
    }
   },
   "outputs": [],
   "source": [
    "best1 = top.iloc[0][['percentile', 'window']].values.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:48:45.873474Z",
     "start_time": "2019-06-16T06:48:45.868720Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, val, test = create_trainval_test(data=data, pctile=best1[0],\n",
    "                                        max_window=best1[1], frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:53:57.640828Z",
     "start_time": "2019-06-16T06:49:19.133550Z"
    }
   },
   "outputs": [],
   "source": [
    "train, val, test = create_trainval_test(data=data, pctile=best1[0],\n",
    "                                        max_window=best1[1], frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:55:27.864216Z",
     "start_time": "2019-06-16T06:55:22.838816Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    " train_tag, val_tag, test_tag) = split(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-16T07:00:14.285Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2228936 samples, validate on 743087 samples\n",
      "Epoch 1/50\n",
      "2097152/2228936 [===========================>..] - ETA: 2s - loss: 0.3880 - mean_squared_error: 0.3880\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.01865, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 53s 24us/step - loss: 0.3661 - mean_squared_error: 0.3661 - val_loss: 0.0187 - val_mean_squared_error: 0.0187\n",
      "Epoch 2/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00002: val_mean_squared_error did not improve from 0.01865\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 3/50\n",
      "2097152/2228936 [===========================>..] - ETA: 1s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00003: val_mean_squared_error did not improve from 0.01865\n",
      "2228936/2228936 [==============================] - 20s 9us/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0333 - val_mean_squared_error: 0.0333\n",
      "Epoch 4/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0539 - mean_squared_error: 0.0539\n",
      "Epoch 00004: val_mean_squared_error did not improve from 0.01865\n",
      "2228936/2228936 [==============================] - 19s 8us/step - loss: 0.0519 - mean_squared_error: 0.0519 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
      "Epoch 5/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00005: val_mean_squared_error did not improve from 0.01865\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0235 - val_mean_squared_error: 0.0235\n",
      "Epoch 6/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00006: val_mean_squared_error improved from 0.01865 to 0.01530, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0153 - val_mean_squared_error: 0.0153\n",
      "Epoch 7/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00007: val_mean_squared_error improved from 0.01530 to 0.01182, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 8/50\n",
      "2097152/2228936 [===========================>..] - ETA: 1s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00008: val_mean_squared_error improved from 0.01182 to 0.00946, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 20s 9us/step - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "Epoch 9/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00009: val_mean_squared_error did not improve from 0.00946\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 10/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0095 - mean_squared_error: 0.0095\n",
      "Epoch 00010: val_mean_squared_error improved from 0.00946 to 0.00710, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 11/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00011: val_mean_squared_error improved from 0.00710 to 0.00476, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 20s 9us/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 12/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 00012: val_mean_squared_error did not improve from 0.00476\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 13/50\n",
      "2097152/2228936 [===========================>..] - ETA: 1s - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 00013: val_mean_squared_error improved from 0.00476 to 0.00304, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 20s 9us/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 14/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 00014: val_mean_squared_error did not improve from 0.00304\n",
      "2228936/2228936 [==============================] - 19s 8us/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 15/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0044 - mean_squared_error: 0.0044\n",
      "Epoch 00015: val_mean_squared_error did not improve from 0.00304\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 16/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0035 - mean_squared_error: 0.0035\n",
      "Epoch 00016: val_mean_squared_error improved from 0.00304 to 0.00303, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 20s 9us/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 17/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0033 - mean_squared_error: 0.0033\n",
      "Epoch 00017: val_mean_squared_error did not improve from 0.00303\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 18/50\n",
      "2097152/2228936 [===========================>..] - ETA: 1s - loss: 0.0033 - mean_squared_error: 0.0033\n",
      "Epoch 00018: val_mean_squared_error improved from 0.00303 to 0.00282, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 20s 9us/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 19/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0028 - mean_squared_error: 0.0028\n",
      "Epoch 00019: val_mean_squared_error did not improve from 0.00282\n",
      "2228936/2228936 [==============================] - 19s 8us/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 20/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0028 - mean_squared_error: 0.0028\n",
      "Epoch 00020: val_mean_squared_error improved from 0.00282 to 0.00280, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 21/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "Epoch 00021: val_mean_squared_error did not improve from 0.00280\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 22/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0028 - mean_squared_error: 0.0028\n",
      "Epoch 00022: val_mean_squared_error did not improve from 0.00280\n",
      "2228936/2228936 [==============================] - 19s 8us/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 23/50\n",
      "2097152/2228936 [===========================>..] - ETA: 1s - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "Epoch 00023: val_mean_squared_error improved from 0.00280 to 0.00214, saving model to model1.hdf5\n",
      "2228936/2228936 [==============================] - 20s 9us/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 24/50\n",
      "2097152/2228936 [===========================>..] - ETA: 0s - loss: 0.0025 - mean_squared_error: 0.0025\n",
      "Epoch 00024: val_mean_squared_error did not improve from 0.00214\n",
      "2228936/2228936 [==============================] - 19s 9us/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 25/50\n",
      " 262144/2228936 [==>...........................] - ETA: 15s - loss: 0.0020 - mean_squared_error: 0.0020"
     ]
    }
   ],
   "source": [
    "model = fit_model('model1.hdf5', X_train, y_train,\n",
    "                  X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T12:35:05.568655Z",
     "start_time": "2019-06-14T12:35:05.507169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0275126235967825,\n",
       " 0.034559915748419066,\n",
       " 0.04001677974234045,\n",
       " 0.044955243093510726,\n",
       " 0.04993596338276819)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# BASELINE MSE OF T + 1\n",
    "b1 = mean_squared_error(test['demand'].values,\n",
    "                        test['demand-1'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 2\n",
    "b2 = mean_squared_error(test['demand'].values,\n",
    "                        test['demand-2'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 3\n",
    "b3 = mean_squared_error(test['demand'].values,\n",
    "                        test['demand-3'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 4\n",
    "b4 = mean_squared_error(test['demand'].values,\n",
    "                        test['demand-4'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 5\n",
    "b5 = mean_squared_error(test['demand'].values,\n",
    "                        test['demand-5'].values)\n",
    "\n",
    "b1, b2, b3, b4, b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T09:06:12.415992Z",
     "start_time": "2019-06-14T09:06:12.362907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039370885265768776"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([b1, b2, b3, b4, b5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
