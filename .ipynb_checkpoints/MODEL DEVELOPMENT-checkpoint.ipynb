{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab A.I. for SEA Challenge: Traffic Management\n",
    "\n",
    "Jayson Yodico <br>\n",
    "Asian Institute of Management\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "It has been well known that understanding congestion dynamics and solving traffic congestion is indispensable to a growing economy. The power of Data Science and Artificial intelligence, with its state-of-the-art techniques, may help in accomplishing this goal.\n",
    "\n",
    "This project seeks to accurately forecast travel demand on a specific area and time based on historical Grab bookings. This project proposes a unique approach of determining performing models among a multitude of possible configuations, which utilizes a combination of machine learning, fourier transform, and genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:21:24.031246Z",
     "start_time": "2019-06-17T07:21:24.015925Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygeohash as gh\n",
    "\n",
    "import scipy\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "*Fields list and descriptions directly lifted from:* https://www.aiforsea.com/traffic-management\n",
    "\n",
    "The raw dataset contains the following fields:\n",
    "\n",
    "- `geohash6`. geohash level 6. Geohash is a public domain geocoding system which encodes a geographic location into a short string of letters and digits with arbitrary precision. You are free to use any geohash library to encode/decode the geohashes into latitude and longitude or vice versa. Some examples include https://github.com/hkwi/python-geohash (for Python), https://github.com/kungfoo/geohash-java (for Java).\n",
    "\n",
    "- `day`. day, where the value indicates the sequential order and not a particular day of the month\n",
    "\n",
    "- `timestamp`. start time of 15-minute intervals, in the following format: <hour>:<minute>, where hour ranges from 0 to 23 and minute is either one of (0, 15, 30, 45)\n",
    "\n",
    "- `demand`. aggregated demand normalised to be in the range [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Upon inspection, it was observed that the are no demand data for some location-time bucket pairs. It is necessary to fill up missing values to make all geohash time series complete.\n",
    "\n",
    "Since it was assumed that all demand data were included in the the timeframe of this training dataset, geohash-time bucket pairs with no demand data are assumed to be zero. This simply means that there is zero demand in that location and time bucket.\n",
    "\n",
    "The purpose of the preprocessing is to fill up gaps in the time series and to make sure that values are arranged chronologically for each location. In addition, decoding of the geohashes  to coordinates (latitude, longitude) are included as additional fields in the processed dataset. The steps are outlined as follows:\n",
    "\n",
    "*Note: The training dataset,* `training.csv`*, must be placed in the folder* `Traffic Management` *of the repository. The dataset can be downloaded from* https://s3-ap-southeast-1.amazonaws.com/grab-aiforsea-dataset/traffic-management.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:21:39.911389Z",
     "start_time": "2019-06-17T07:21:26.510338Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('Traffic Management/training.csv')\n",
    "raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'], format= '%H:%M').dt.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create dummy datetime values to serve as reference in ordering the `day` and `timestamp` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:23:02.364726Z",
     "start_time": "2019-06-17T07:23:00.411407Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = pd.DataFrame()\n",
    "dates['dummy_date'] = pd.date_range(start=pd.datetime(2019, 1, 1),\n",
    "                              periods=len(raw_df.day.unique())+1)\n",
    "dates['day'] = np.arange(1, dates.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With the dummy datetime values, the sequence can be ordered. For each location, `T_n` serves as an ID indicating the order a value appears in that location-time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:23:05.432296Z",
     "start_time": "2019-06-17T07:23:03.995116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>T_n</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:15:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:30:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:45:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp  T_n  day\n",
       "0  00:00:00    0    1\n",
       "1  00:15:00    1    1\n",
       "2  00:30:00    2    1\n",
       "3  00:45:00    3    1\n",
       "4  01:00:00    4    1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a reference table of chronological ordering indices (T_n) mapped to\n",
    "timestamp and day values.\n",
    "\"\"\"\n",
    "\n",
    "timenum = pd.DataFrame(pd.date_range(start=dates.dummy_date.min(),\n",
    "                                     end=dates.dummy_date.max(),\n",
    "                                     freq='15min'), columns=['dummy_datetime'])\n",
    "\n",
    "timenum['dummy_date'] = pd.to_datetime(timenum['dummy_datetime'].dt.date)\n",
    "timenum['timestamp'] = timenum['dummy_datetime'].dt.time\n",
    "\n",
    "timenum['T_n'] = np.arange(timenum.shape[0])\n",
    "timenum = timenum.merge(dates, on='dummy_date', how='left')\n",
    "\n",
    "del timenum['dummy_datetime'], timenum['dummy_date']\n",
    "timenum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:23:08.964884Z",
     "start_time": "2019-06-17T07:23:07.215009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>day</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>demand</th>\n",
       "      <th>T_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp03wc</td>\n",
       "      <td>18</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>1712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp03pn</td>\n",
       "      <td>10</td>\n",
       "      <td>14:30:00</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp09sw</td>\n",
       "      <td>9</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp0991</td>\n",
       "      <td>32</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>0.088755</td>\n",
       "      <td>2996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp090q</td>\n",
       "      <td>15</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6  day timestamp    demand   T_n\n",
       "0   qp03wc   18  20:00:00  0.020072  1712\n",
       "1   qp03pn   10  14:30:00  0.024721   922\n",
       "2   qp09sw    9  06:15:00  0.102821   793\n",
       "3   qp0991   32  05:00:00  0.088755  2996\n",
       "4   qp090q   15  04:00:00  0.074468  1360"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merging the table with the original dataset\n",
    "\"\"\"\n",
    "\n",
    "df = raw_df.merge(timenum, on=['day', 'timestamp'], how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Filling in gaps in the time series with zeroes (i.e. no demand at that time and location) and decoding the geohashes into latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:23:38.007285Z",
     "start_time": "2019-06-17T07:23:26.650276Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fill gaps in each geohash time series with zeroes\n",
    "\"\"\"\n",
    "g = df.groupby(['geohash6'])\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for loc in g.groups.keys():\n",
    "    \n",
    "    test = g.get_group(loc)\n",
    "\n",
    "    dummy = timenum[(timenum.T_n >= test.T_n.min())\n",
    "                    & (timenum.T_n <= test.T_n.max())]\n",
    "    \n",
    "    # Fill gaps with zeroes\n",
    "    dummy = dummy.merge(test[['T_n', 'demand']], on='T_n', how='left').fillna(0)\n",
    "    dummy['geohash6'] = loc\n",
    "    dummy['lat'] = gh.decode(loc)[0]\n",
    "    dummy['long'] = gh.decode(loc)[1]\n",
    "    \n",
    "    all_data.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:23:43.131166Z",
     "start_time": "2019-06-17T07:23:40.116248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>T_n</th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>geohash6</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02:45:00</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03:15:00</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03:30:00</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03:45:00</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp  T_n  day    demand geohash6   lat  long\n",
       "0  02:45:00   11    1  0.020592   qp02yc -5.48  90.7\n",
       "1  03:00:00   12    1  0.010292   qp02yc -5.48  90.7\n",
       "2  03:15:00   13    1  0.000000   qp02yc -5.48  90.7\n",
       "3  03:30:00   14    1  0.000000   qp02yc -5.48  90.7\n",
       "4  03:45:00   15    1  0.000000   qp02yc -5.48  90.7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat(all_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Saving the processed file. This will be used to create models in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:24:36.630475Z",
     "start_time": "2019-06-17T07:23:45.271345Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('Traffic Management/training_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process ensures that timestamps for every geohash is put together to create a geohash-time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Development\n",
    "\n",
    "This project proposes a unique approach of determining a potentially performing model among a multitude of possible configuations. For instance, it is of interest to know how many periods of past values can be used to predict future values.\n",
    "\n",
    "The model development process is centered around three processes:\n",
    "\n",
    "- **Denoising.** Applying a certain level of denoising using Fourier transform\n",
    "- **Windowing**. Given historical values ($T_{n},T_{n-1},.., T_{n-k}$), predict future values ($T_{n+1},T_{n+2}.., T_{n+5}$), where $k$ is the window.\n",
    "- **Genetic Algorithm**. There are many possible model configurations if the data is to be feature-engineered in terms of denoising and windowing. The conventional approach is to use grid search or randomized search in finding the best denoising-windowing configuration. To make the search process more efficient and systematic, this project proposes to use genetic algorithm to find the best denoising-windowing configuration.\n",
    "- **Model Fitting and Performance.** Each window-percentile combination is modeled using a 3-layer neural network model. A training dataset is fitted (first 60% of each geohash time series) and validated using a validation dataset (succeeding 20%).\n",
    "\n",
    "The details of the techniques used in this study are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising using Fourier Transform\n",
    "\n",
    "The first step is to denoise each geohash time series using Fourier transform. Used in signal processing, it postulates that any waveform whose value varies with time can be decomposed into simple waveforms of varying frequencies. From the time domain, the series is projected into the frequency domain. Take for example a geohash time series:\n",
    "\n",
    "<img src=\"sample_series.png\" width=\"1500\" />\n",
    "\n",
    "in the time domain projected to the frequency domain. The image below represents the distribution of intensities of the frequency components that make up the time series.\n",
    "\n",
    "<img src=\"projected series.png\" width=\"700\" />\n",
    "\n",
    "Fourier transform works as a denoising method by silencing the components with low intensities. In principle, these are the 'less important' components that may just contribute to unexplainable noise in the time series.\n",
    "\n",
    "From these component frequencies, a threshold could be defined in terms of a percentile value $q$, such that intensities lower than $q$ is set to zero. This effectively removes the component frequencies that may contribute to the noise in the dataset. Higher levels of $q$ means a higher level of denoising. For example, when intensities below the 90th percentile are silenced, the resulting spectrogram is shown below:\n",
    "\n",
    "<img src=\"projected series silenced.png\" width=\"700\" />\n",
    "\n",
    "This silenced spectrogram, when brought back to the time domain, becomes the following series.\n",
    "\n",
    "<img src=\"cleaned_series.png\" width=\"1500\" />\n",
    "\n",
    "Notice how the denoised series is approximately the shape of the original series, but is less fluctuating. However, too much denoising could ignore the explainable fluctuations of a time series, such that when fitted to a machine learning model, could make less powerful predictions.\n",
    "\n",
    "In this sense, a parameter to be tuned in this process is a *percentile value* denoting the level of denoising. The function that executes this process is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:17:08.786029Z",
     "start_time": "2019-06-17T02:17:08.769820Z"
    }
   },
   "outputs": [],
   "source": [
    "def denoise(series, pctile):\n",
    "\n",
    "    \"\"\"\n",
    "    Denoises a time series by projecting the series to the frequency domain and\n",
    "    silencing frequencies less than the frequency threshold.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        series: DataFrame\n",
    "            - series to denoised\n",
    "\n",
    "        pctile: int\n",
    "            - percentile threshold of denoising. In the frequency domain, all\n",
    "              frequency components with intensities less than the threshold is\n",
    "              set to zero. Higher percentile values means higher level of\n",
    "              denoising.\n",
    "          \n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "        cleaned_series: ndarray\n",
    "            - denoised series\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sff = scipy.fft(series)    \n",
    "    abs_sff = abs(sff)\n",
    "    sff[abs_sff < np.percentile(abs_sff, q=pctile)] = 0\n",
    "    cleaned_series = np.abs(scipy.ifft(sff))\n",
    "\n",
    "    return cleaned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "\n",
    "It is interesting to take note that traffic volume has a strong temporal characteristic. Its strong temporal characteristic is rooted on the fact that it reflects peoples day-to-day activities. People typically go to work in the morning and leaves work before evening. This strongly sugggests that **traffic volume has a daily periodicity.**\n",
    "\n",
    "The denoised series will be fitted to a machine learning model. The features are the latitude and longtitude, and \n",
    "past $n$ values. It must be noted, however, that the predictions will still be compared to the original time series.\n",
    "\n",
    "It is also important to get the next 15-minute buckets since the goal of the project is to predict these future values.\n",
    "\n",
    "As mentioned earlier, each geohash is treated as its own time series. The code below iterates over each geohashes to:\n",
    "- denoise every geohash time series\n",
    "- get the future denoised values (for model fitting) and the actual future values (for model performance)\n",
    "- get the historical values as features\n",
    "- if there is a lack of computational resources, the user may opt to take a small sample of the derived data points. It must be emphasized that this sampling procedure does not break the time series since the previous and future values has already been appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:10:36.581739Z",
     "start_time": "2019-06-17T07:10:36.341058Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_trainval_test(data, pctile, max_window, frac=0.001):\n",
    "    \n",
    "    \"\"\"\n",
    "    Iterates over each geohash to extract past and future values.\n",
    "    Once done, it splits the dataset to training, validation, and\n",
    "    test set on a 60-20-20 split\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        data: pandas.DataFrame\n",
    "            - the preprocessed dataset that contains the geohashes,\n",
    "            coordinates, and demand\n",
    "\n",
    "        pctile: int\n",
    "            - percentile threshold of denoising. This value is passed in the\n",
    "            `denoise` function\n",
    "\n",
    "        max_window: int\n",
    "            - number of past 15-bucket pairs to be used as features\n",
    "\n",
    "        frac: float, 0 to 1\n",
    "            - percent of data points to be sampled (after features and targets)\n",
    "            have been extracted\n",
    "    \n",
    "    RETURNS:\n",
    "        train, val, test: pandas.DataFrame\n",
    "            - the training, validation, and test datasets respectively\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    g = data.groupby('geohash6')\n",
    "    \n",
    "    # Iterate over each geohash\n",
    "    for each_gh in g.groups.keys():\n",
    "        dummy = g.get_group(each_gh).copy()\n",
    "        dummy['demand_fft'] = denoise(dummy.demand.values, pctile)\n",
    "\n",
    "        # extract future values of actual and denoised series\n",
    "        for fwd in range(-2,-6,-1):\n",
    "            dummy[f'demand+{-1*fwd}'] = dummy['demand'].shift(fwd+1)\n",
    "            dummy[f'demand_fft+{-1*fwd}'] = dummy['demand_fft'].shift(fwd+1)\n",
    "\n",
    "        # extract past 5 time buckets of the actual dataset\n",
    "        for bwd in range(1, 6):\n",
    "            dummy[f'demand-{bwd}'] = dummy['demand'].shift(bwd)\n",
    "\n",
    "        # extract past n time buckets based on max_window parameter\n",
    "        for bwd in range(1, max_window + 1):\n",
    "            dummy[f'demand_fft-{bwd}'] = dummy['demand_fft'].shift(bwd)\n",
    "\n",
    "        # Remove rows with no historical data\n",
    "        dummy = dummy.dropna()\n",
    "        len_dummy = dummy.shape[0]\n",
    "        \n",
    "        # split in train, validation, and test\n",
    "        sp1, sp2 = int(len_dummy*0.6), int(len_dummy*0.8)\n",
    "        trn = dummy.iloc[:sp1,:].sample(frac=frac)\n",
    "        vl = dummy.iloc[sp1:sp2,:].sample(frac=frac)\n",
    "        tst = dummy.iloc[sp2:,:].sample(frac=frac)\n",
    "\n",
    "        # creates a file in the beginning, adds to file in the succeeding\n",
    "        # iterations\n",
    "        if i == 0:\n",
    "            trn.to_csv('train1.csv', index=False)\n",
    "            vl.to_csv('val1.csv', index=False)\n",
    "            tst.to_csv('test1.csv', index=False)\n",
    "\n",
    "        else:\n",
    "            trn.to_csv('train1.csv', mode='a', header=False, index=False)\n",
    "            vl.to_csv('val1.csv', mode='a', header=False, index=False)\n",
    "            tst.to_csv('test1.csv', mode='a', header=False, index=False)  \n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    # reads completed files and returns the values\n",
    "    train = pd.read_csv('train1.csv')\n",
    "    val = pd.read_csv('val1.csv')\n",
    "    test = pd.read_csv('test1.csv')\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:10:37.924796Z",
     "start_time": "2019-06-17T07:10:37.859495Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(train, val, test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Splits the train, validation, and test datasets into features and targets.\n",
    "\n",
    "    PARAMETERS:\n",
    "        train, val, test: pandas.DataFrame\n",
    "            - the training, validation, and test datasets respectively\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "        X_train, y_train, X_val, y_val, X_test, y_test: pandas.DataFrame\n",
    "            - training, validation and test datasets splitted into\n",
    "             features and targets\n",
    "\n",
    "        train_tag, val_tag, test_tag:\n",
    "            - geohash tags for each corresponding datapoint\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    toremove_feat = ['demand','demand+2', 'demand+3','demand+4', 'demand+5',\n",
    "                     'demand_fft','demand_fft+2', 'demand_fft+3','demand_fft+4',\n",
    "                     'demand_fft+5','demand-1', 'demand-2', 'demand-3',\n",
    "                     'demand-4','demand-5', 'geohash6']\n",
    "\n",
    "    targets = ['demand_fft', 'demand_fft+2', 'demand_fft+3', 'demand_fft+4',\n",
    "               'demand_fft+5']\n",
    "    \n",
    "    tags = ['geohash6']\n",
    "\n",
    "    targets_actual = ['demand', 'demand+2', 'demand+3', 'demand+4', 'demand+5']\n",
    "\n",
    "    X_train = train.drop(toremove_feat, axis=1)\n",
    "    y_train = train[targets]\n",
    "    train_tag = train[tags]\n",
    "\n",
    "    X_val = val.drop(toremove_feat, axis=1)\n",
    "    y_val = val[targets]\n",
    "    val_tag = val[tags]\n",
    "\n",
    "    X_test = test.drop(toremove_feat, axis=1)\n",
    "    y_test = test[targets_actual]\n",
    "    test_tag = test[tags]\n",
    "    \n",
    "    return (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "            train_tag, val_tag, test_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "The model is fitted on a 3-layer neural network (1 output layer, 1 hidden layer, 1 output layer). The model gives out five outputs which correspond to the next 5 future time buckets. The model learns by minimizing the mean squared error accross time buckets. \n",
    "\n",
    "The hidden layers are configured such that the number of hidden nodes is five times the number of features. The activation function in the input and output layer is linear, while the hidden uses the rectified linear unit function (ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:10:40.295038Z",
     "start_time": "2019-06-17T07:10:40.207823Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(filepath, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits the dataset on a neural network model.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        filepath: str\n",
    "            - filepath of the fitted model in .hdf5 format\n",
    "\n",
    "        X_train, y_train, X_val, y_val: pandas.DataFrame\n",
    "            - the training and validation datasets respectively\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "        model: keras.model\n",
    "            - a the fitted model\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    n_rows, n_cols = X_train.shape\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_cols, input_dim = n_cols, kernel_initializer='uniform',\n",
    "                    activation='linear'))\n",
    "    model.add(Dense(n_cols*5, kernel_initializer='uniform',\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(5, kernel_initializer='uniform', activation='linear'))\n",
    "    \n",
    "    lrs = [0.001, 0.0001]\n",
    "    patience_vals = [20, 200]\n",
    "    epoch_vals = [100, 1000]\n",
    "    \n",
    "    for ind in range(len(lrs)):\n",
    "    \n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=optimizers.RMSprop(lr=lrs[ind]),\n",
    "                      metrics=['mean_squared_error'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath, monitor='val_mean_squared_error',\n",
    "                                 verbose=0, save_best_only=True,\n",
    "                                 mode='min')\n",
    "\n",
    "        es = EarlyStopping(monitor='val_mean_squared_error',\n",
    "                           patience=patience_vals[ind], verbose=0,\n",
    "                           min_delta=0.00001)\n",
    "\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                  epochs=epoch_vals[ind], batch_size=1024*32,\n",
    "                  callbacks=[es, mc],\n",
    "                  verbose=0)\n",
    "\n",
    "        model = load_model(filepath)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "The model is evaluated in terms of mean squared error (MSE). Take note that even if the denoised series was predicted to the model, the predictions are still compared to the original values.\n",
    "\n",
    "There are two modes for model evaluation. The user may opt to evaluate the overall performance, which is the average MSE accross all target values. Another option is that the user may also view the individual MSEs of each time bucket predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:14:46.227310Z",
     "start_time": "2019-06-17T07:14:46.142629Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluates overall MSE across time buckets.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        model: keras.model\n",
    "            - filepath of the fitted model in .hdf5 format\n",
    "\n",
    "        X_test, y_test: pandas.DataFrame\n",
    "            - the test datasets. Predictions in `X_test` (denoised series)\n",
    "            is compared to the `y_test` (actual series)\n",
    "\n",
    "    RETURNS:\n",
    "    \n",
    "        fitness:  float\n",
    "            - mean squared errors of each time bucket pairs\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    a1 = mean_squared_error(preds[:,0].ravel(), y_test['demand'].values)\n",
    "    a2 = mean_squared_error(preds[:,1].ravel(), y_test['demand+2'].values)\n",
    "    a3 = mean_squared_error(preds[:,2].ravel(), y_test['demand+3'].values)\n",
    "    a4 = mean_squared_error(preds[:,3].ravel(), y_test['demand+4'].values)\n",
    "    a5 = mean_squared_error(preds[:,4].ravel(), y_test['demand+5'].values)\n",
    "\n",
    "    fitness = np.mean([a1, a2, a3, a4, a5])\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "def evaluate_indiv_testmse(model, X_test, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluates MSE for each time bucket.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        model: keras.model\n",
    "            - filepath of the fitted model in .hdf5 format\n",
    "\n",
    "        X_test, y_test: pandas.DataFrame\n",
    "            - the test datasets. Predictions in `X_test` (denoised series)\n",
    "            is compared to the `y_test` (actual series)\n",
    "\n",
    "    RETURNS:\n",
    "    \n",
    "        a1, a2, a3, a4, a5: float\n",
    "            - mean squared errors of each time buckets\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    a1 = mean_squared_error(preds[:,0].ravel(), y_test['demand'].values)\n",
    "    a2 = mean_squared_error(preds[:,1].ravel(), y_test['demand+2'].values)\n",
    "    a3 = mean_squared_error(preds[:,2].ravel(), y_test['demand+3'].values)\n",
    "    a4 = mean_squared_error(preds[:,3].ravel(), y_test['demand+4'].values)\n",
    "    a5 = mean_squared_error(preds[:,4].ravel(), y_test['demand+5'].values)\n",
    "\n",
    "    return a1, a2, a3, a4, a5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENETIC ALGORITHM FUNCTIONS\n",
    "\n",
    "The goal of the model development process is to create a model that has the ability to genralize (i.e. predict future or unseen values with minimal error). Every machine learning model has a certain set of hyperparameters and these needs to be properly tuned to prevent underfitting or overfitting. Aside from the weights of the neural network model, **level of denosing (percentile) and number of historical values to use as features (window)**.\n",
    "\n",
    "There are many possible model configurations if the data is to be feature-engineered in terms of denoising and windowing. The conventional approach is to use grid search or randomized search in finding the best denoising-windowing configuration. To make the search process more efficient and systematic, this project proposes to use genetic algorithm to find the best denoising-windowing configuration.\n",
    "\n",
    "1. Candidates. The algorithm begins with an initial set of candidates, which is a randomly selected set of window-percentile combinations. The number of initial candidates is predetermined by the user. The term percentile refers to the top component sinusoids with the highest frequencies to be selected for the Fourier transform. On the other hand, the window refers to the number of previous weekly Fourier-transformed sales to be used as features for the model. The percentile configurations are from 5 to 95 (top 95% to 5%) and window configurations are from 15 to 96 minutes (15 minutes to 24 hours). Being limited in this range is rooted on the daily periodicity of traffic volume.\n",
    "\n",
    "2. Calculation of fitness score. The fitness score is based on the overall test MSE of each window-percentile configuration. The model is fitted using the training and validation datasets. The model is evaluated based on its ability to predict the unseen test dataset.\n",
    "\n",
    "3. Selection of fittest candidates. The window-percentile combinations with the highest test MSEs are selected for crossover. The user may select this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:08:56.536363Z",
     "start_time": "2019-06-17T04:08:56.495790Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_testmse(data, pctile, window):\n",
    "\n",
    "    \"\"\"\n",
    "    Implements the model development pipeline.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        data: pandas.DataFrame\n",
    "            - the preprocessed dataset that contains the geohashes,\n",
    "            coordinates, and demand\n",
    "\n",
    "        pctile: int\n",
    "            - percentile threshold of denoising. This value is passed in the\n",
    "            `denoise` function\n",
    "\n",
    "        window: int\n",
    "            - number of past 15-bucket pairs to be used as features\n",
    "\n",
    "    RETURNS:\n",
    "    \n",
    "        fitness:  float\n",
    "            - mean squared errors of each time buckets\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    train, val, test = create_trainval_test(data=data, pctile=pctile,\n",
    "                                            max_window= window)\n",
    "    \n",
    "    print('Creating Data: Done', end='\\t')\n",
    "    \n",
    "    (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "     train_tag, val_tag, test_tag) = split(train, val, test)\n",
    "    \n",
    "    model = fit_model('testmodel.hdf5', X_train, y_train,\n",
    "                      X_val, y_val)\n",
    "    \n",
    "    print('Model Fitting: Done', end='\\t')\n",
    "    \n",
    "    fitness = evaluate(model, X_test, y_test)\n",
    "    \n",
    "    print(f'Fitness = {fitness}')\n",
    "    \n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Crossover. The fittest candidates are paired to produce a new set of candidates that acquire the attributes of the fittest candidates. The fittest candidate in the previous generation clones itself to the next generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:19:17.536170Z",
     "start_time": "2019-06-17T04:19:17.489495Z"
    }
   },
   "outputs": [],
   "source": [
    "def mate(fittest, mating_pairs, n_children):\n",
    "    \n",
    "    \"\"\"\n",
    "    Crossovers the fittest candidates determined in the first generation.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "       fittest: list\n",
    "            - the fittest candidate containing its percentile window\n",
    "            configuration\n",
    "\n",
    "        mating_pairs: list of tuples\n",
    "            - possible pairings of the fittest candidates\n",
    "\n",
    "        n_children: int\n",
    "            - number of children to be created in the crossover. Must be\n",
    "            the same as the number of initial species evaluated\n",
    "\n",
    "    RETURNS:\n",
    "    \n",
    "        species: ndarray\n",
    "            - children produced in the crossover to be evaluated in the\n",
    "            next generation\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    species = np.zeros((n_species, len_gene), dtype=int)\n",
    "    species[0] = fittest\n",
    "\n",
    "    i = 1   \n",
    "    for father, mother in mating_pairs: \n",
    "        \n",
    "        parents = [father, mother]\n",
    "        np.random.shuffle(parents)\n",
    "        \n",
    "        child = np.zeros(len_gene, dtype=int)\n",
    "        \n",
    "        child[:1] = parents[0][:1]\n",
    "        child[1:] = parents[1][1:]\n",
    "\n",
    "        species[i] = child\n",
    "        i += 1\n",
    "        if i == n_children: break\n",
    "\n",
    "    return species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Mutation. Once the crossover is done, each of the window-percentile configurations of the new candidates can be randomly modified. Mutation encourages the search process to look for other window-percentile combinations that may improve the solution, thus, reducing the risk of being trapped in a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:20:51.799168Z",
     "start_time": "2019-06-17T04:20:51.771771Z"
    }
   },
   "outputs": [],
   "source": [
    "def mutate(species, mutation_rate):\n",
    "    \n",
    "    \"\"\"\n",
    "    Mutates the species to look for more combinations that may improve\n",
    "    the solution.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "       species: ndarray\n",
    "            - candidate to be altered prior evaluation\n",
    "\n",
    "       mutation rate: float from 0 to 1\n",
    "            - higher mutation rate means higher chances of a specie being\n",
    "            altered\n",
    "\n",
    "    RETURNS:\n",
    "    \n",
    "        species: ndarray\n",
    "            - altered candidates\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    n_species = species.shape[0]\n",
    "    n_mutate = int(mutation_rate*(n_species - 1))\n",
    "\n",
    "    inds_mut = np.random.randint(1, n_species, size=n_mutate)\n",
    "    species[inds_mut, 0] = np.random.choice(np.arange(min_pctile, max_pctile, 5), n_mutate)\n",
    "\n",
    "    inds_mut = np.random.randint(1, n_species, size=n_mutate)\n",
    "    species[inds_mut, 1] = np.random.choice(np.arange(min_window, max_window, 4), n_mutate)\n",
    "   \n",
    "    return species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Diversification. This process is similar to mutation. The crossover process may produce a candidate that has been evaluated in the previous generations and the current generation, which is a waste of computational resources. In the genetic algorithm, the model alters either the window or percentile value until it becomes a new candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:36:15.732781Z",
     "start_time": "2019-06-17T04:36:15.712073Z"
    }
   },
   "outputs": [],
   "source": [
    "def diversify(sp):\n",
    "    \n",
    "    \"\"\"\n",
    "    Alters a candidate when it has been already evaluated in the previous\n",
    "    generations.\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        sp: list or ndarray\n",
    "            - candidate to be altered\n",
    "\n",
    "    RETURNS:\n",
    "    \n",
    "        sp: list or ndarray\n",
    "            - altered candidate\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    col_to_change = np.random.choice(list(range(len_gene)))\n",
    "    \n",
    "    if col_to_change == 0:\n",
    "        sp[0] = np.random.choice(np.arange(min_pctile, max_pctile, 5),1)# PERCENTILE\n",
    "    \n",
    "    elif col_to_change == 1:\n",
    "        sp[1] = np.random.choice(np.arange(min_window, max_window, 4), 1)\n",
    "    \n",
    "    return sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Repetition. The process is repeated in a prespecified number of iterations. The evaluated candidates and their respective fitness values are recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:43:00.695173Z",
     "start_time": "2019-06-17T04:43:00.672513Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_to_records(all_records, species, fitness_values):\n",
    "\n",
    "    \"\"\"\n",
    "    Records the results from the current generation\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "        all records: pandas.DataFrame\n",
    "            - records of previous generations\n",
    "            \n",
    "        species: ndarray\n",
    "            - species evaluated in the current generation\n",
    "            \n",
    "        fitness_values: list\n",
    "            - fitness values in the same order as the evaluation of the\n",
    "            species\n",
    "\n",
    "    RETURNS:\n",
    "    \n",
    "        all records: pandas.DataFrame\n",
    "            - records with appended records of the current generation\n",
    "        \n",
    "    \"\"\"    \n",
    "    \n",
    "    res_gen = pd.DataFrame(species, columns=['percentile', 'window'])\n",
    "    res_gen['gen'] = gen\n",
    "    res_gen['index'] = list(range(n_species))\n",
    "    res_gen['fitness_test_mse'] = fitness_values\n",
    "    \n",
    "    all_records = pd.concat([all_records, res_gen])\n",
    "    all_records.to_csv('all_records.csv', index=False)\n",
    "    \n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm Implementation\n",
    "\n",
    "In the implementation of the genetic algorithm, start with reading the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:45:17.197135Z",
     "start_time": "2019-06-16T03:44:54.866214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.020592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.010292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>-5.48</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6   lat  long    demand\n",
       "0   qp02yc -5.48  90.7  0.020592\n",
       "1   qp02yc -5.48  90.7  0.010292\n",
       "2   qp02yc -5.48  90.7  0.000000\n",
       "3   qp02yc -5.48  90.7  0.000000\n",
       "4   qp02yc -5.48  90.7  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Traffic Management/training_processed.csv')\n",
    "df = df.sort_values(['geohash6', 'T_n'])\n",
    "\n",
    "del df['timestamp'], df['T_n'], df['day']\n",
    "\n",
    "data = df[['geohash6', 'lat', 'long', 'demand']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the user-defined values that are configured prior to the start of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:49:02.658810Z",
     "start_time": "2019-06-17T04:49:02.615231Z"
    }
   },
   "outputs": [],
   "source": [
    "res_file = 'all_records.csv' # Filename of genetic algorithm records\n",
    "exists = os.path.isfile(res_file)\n",
    "\n",
    "# number of species\n",
    "n_species = 10 \n",
    "\n",
    "# number of attribute of each specie. In this case, there are two attributes:\n",
    "#     percentile and window\n",
    "len_gene = 2 \n",
    "gene_names = ['percentile', 'window']\n",
    "\n",
    "# number of generations\n",
    "max_gen = 10\n",
    "\n",
    "# mutation rates. Mutation increase as the algorithm progresses\n",
    "#    over generations\n",
    "mut_rates = np.linspace(0, 0.5, max_gen)\n",
    "\n",
    "# number of fittest species to be selected for crossover\n",
    "n_select = 5\n",
    "\n",
    "# possible values of window-percentile configurations\n",
    "min_pctile, max_pctile = 5, 100\n",
    "min_window, max_window = 4, 100\n",
    "\n",
    "# column names of records  file for reference\n",
    "col_names = ['percentile', 'window', 'gen', 'index', 'fitness_test_mse']\n",
    "\n",
    "# this records the tested species as the algorithm progresses. The value here\n",
    "#    is just a placeholder\n",
    "tested_species = np.array([-99,-99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initializes the genetic algorithm. When a run is interrupted due to due to unseen circumstances, the user may continue running the GA from most recent generation using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T06:08:03.292718Z",
     "start_time": "2019-06-17T06:08:03.171044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA Records File has been completed\n"
     ]
    }
   ],
   "source": [
    "if exists:\n",
    "    \n",
    "    try:\n",
    "        results_df = pd.read_csv(res_file)\n",
    "        tested_species = results_df[gene_names].values\n",
    "        recent_results = results_df[results_df['gen'] == max(results_df['gen'])][col_names].reset_index(drop=True)\n",
    "\n",
    "        num_generations = list(range(max(results_df['gen'])+1, max_gen))\n",
    "\n",
    "        mating_pool = recent_results[gene_names].values[:n_select]\n",
    "        mating_pairs = list(itertools.combinations(mating_pool, 2))    \n",
    "        np.random.shuffle(mating_pairs)    \n",
    "\n",
    "        recent_results = recent_results.sort_values('fitness_test_mse')\n",
    "        fittest_val = recent_results.iloc[0]['fitness_test_mse']\n",
    "        fittest = recent_results.iloc[0][gene_names].values.astype(int)\n",
    "\n",
    "        species = mate(fittest=fittest, mating_pairs=mating_pairs,\n",
    "                   n_children=n_species)\n",
    "        species = mutate(species, mutation_rate=mut_rates[num_generations[0]])\n",
    "\n",
    "        tested_species = results_df[gene_names].values\n",
    "        \n",
    "    except IndexError:\n",
    "        print('GA Records File has been completed')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    num_generations = list(range(0,max_gen))\n",
    "    results_df = pd.DataFrame(columns=col_names)\n",
    "    \n",
    "    # INITIALIZE VALUES\n",
    "    species = np.zeros((n_species, len_gene), dtype=int)\n",
    "    species[:,0] = np.random.choice(np.arange(min_pctile, max_pctile, 5), n_species)\n",
    "    species[:,1] = np.random.choice(np.arange(min_window, max_window, 4), n_species)    \n",
    "    tested_species = np.array([-99,-99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specie convention\n",
    "\n",
    "In this program, the species are denoted as a 2D numpy array. For each row there are two columns. For example the specie\n",
    "<br>\n",
    "`[45  92]`\n",
    "\n",
    "means to denoise the time series using the 45th percentile threshold and use the previous 92 values of the denoised series to predict future values.\n",
    "\n",
    "It is recommended to run the model on a small sample of data points. Again, the data points and its corresponding features and targets must be derived first prior sampling.\n",
    "\n",
    "The code below runs the genetic algorithm from a predefined number of generations. The output below shows a resumed run from the 8th generation (generation index 7) up to the 10th generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:19:39.297946Z",
     "start_time": "2019-06-16T03:45:17.364877Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation 7:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[30 44] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003377516621380486\n",
      "[75 12] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003433179373105824\n",
      "[85 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0033305781445787884\n",
      "[ 5 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003343668680430529\n",
      "[95 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004323591796865457\n",
      "[75 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004525261417953867\n",
      "[45 32] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0027725289104957983\n",
      "[55 80] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003492590125429041\n",
      "[85  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024145148705954724\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n",
      "generation 8:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[25  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025493469318358325\n",
      "[60 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024812099327977976\n",
      "[90 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025470279941015974\n",
      "[40 52] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0033379552193915085\n",
      "[85  4] Creating Data: Done\tModel Fitting: Done\tFitness = 0.014006311209603255\n",
      "[35  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.004556459956561222\n",
      "[ 5 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.002309556548289862\n",
      "[15 12] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0029374550857402893\n",
      "[15 36] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0031429835664478767\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n",
      "generation 9:\n",
      "[45 92] Done\tFitness = 0.0019635655589860857\n",
      "[55 92] Creating Data: Done\tModel Fitting: Done\tFitness = 0.002108363880202165\n",
      "[75 28] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0032650283261267594\n",
      "[ 5 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.001974336284176592\n",
      "[10 88] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0024385333057059745\n",
      "[20 96] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0025742614876441547\n",
      "[65  8] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003055786612043888\n",
      "[35 68] Creating Data: Done\tModel Fitting: Done\tFitness = 0.003112448435805511\n",
      "[5 4] Creating Data: Done\tModel Fitting: Done\tFitness = 0.015855070100878832\n",
      "[60 72] Creating Data: Done\tModel Fitting: Done\tFitness = 0.0034283913681592663\n",
      "Fittest:[45 92] 0.0019635655589860857\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for gen in num_generations:\n",
    "    \n",
    "    print(f'generation {gen}:')\n",
    "    fitness_values = []\n",
    "    \n",
    "    for j, sp in enumerate(species):\n",
    "\n",
    "        if gen == 0:\n",
    "            print(sp, end=' ')\n",
    "            fitness = calculate_testmse(data, sp[0], sp[1])   \n",
    "            tested_species = np.vstack([tested_species, sp])\n",
    "\n",
    "        elif gen > 0:\n",
    "            if j == 0:\n",
    "                print(sp, 'Done', end='\\t')\n",
    "                fitness = fittest_val\n",
    "                print(f'Fitness = {fitness}')\n",
    "\n",
    "            elif j > 0:\n",
    "                while sp.tolist() in tested_species.tolist():\n",
    "                    sp = diversify(sp)\n",
    "                print(sp, end=' ')\n",
    "                species[j,:] = sp\n",
    "                fitness = calculate_testmse(data, sp[0], sp[1])    \n",
    "                tested_species = np.vstack([tested_species, sp])\n",
    "\n",
    "        fitness_values.append(fitness)\n",
    "\n",
    "    res_gen = pd.DataFrame(species, columns=gene_names)\n",
    "    res_gen['gen'] = gen\n",
    "    res_gen['index'] = list(range(n_species))\n",
    "    res_gen['fitness_test_mse'] = fitness_values\n",
    "    \n",
    "    results_df = pd.concat([results_df, res_gen])\n",
    "    results_df.to_csv(res_file, index=False)\n",
    "\n",
    "    fittest, fittest_val = (species[np.argmin(fitness_values)],\n",
    "                            np.min(fitness_values))\n",
    "    \n",
    "    print(f'Fittest:{fittest} {fittest_val}')\n",
    "    print('==================================')\n",
    "    \n",
    "    species = species[np.argsort(fitness_values)]\n",
    "    mating_pool = species[:n_select]\n",
    "    mating_pairs = list(itertools.combinations(mating_pool, 2))    \n",
    "    np.random.shuffle(mating_pairs)\n",
    "    \n",
    "    species = mate(fittest=fittest, mating_pairs=mating_pairs,\n",
    "                   n_children=n_species)\n",
    "    \n",
    "    species = mutate(species, mutation_rate=mut_rates[gen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, it can be seen that the specie `[45 92]` has the best generalization error. Using this value, a model was fitted on a larger value (sampled 50% from the total data points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Error\n",
    "\n",
    "The model has to beat a baseline error. For a time series model, the conventional baseline error is the error from the persistence method. The forecast for tommorrow is the same as today. The forecast two days from now is the same as today, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T06:08:50.272703Z",
     "start_time": "2019-06-17T06:08:22.456744Z"
    }
   },
   "outputs": [],
   "source": [
    "g = data.groupby('geohash6')\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for each_gh in g.groups.keys():\n",
    "    dummy = g.get_group(each_gh).copy()\n",
    "    \n",
    "    for bwd in range(1, 6):\n",
    "        dummy[f'demand-{bwd}'] = dummy['demand'].shift(bwd)\n",
    "        \n",
    "    dummy = dummy.dropna()\n",
    "    \n",
    "    all_dfs.append(dummy)\n",
    "    \n",
    "data2 = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual baseline errors are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T06:08:52.193038Z",
     "start_time": "2019-06-17T06:08:51.095455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0006979268923042981,\n",
       " 0.0010812851416353638,\n",
       " 0.001417649086463842,\n",
       " 0.0017633287859278506,\n",
       " 0.0021435275081951254)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASELINE MSE OF T + 1\n",
    "b1 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-1'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 2\n",
    "b2 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-2'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 3\n",
    "b3 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-3'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 4\n",
    "b4 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-4'].values)\n",
    "\n",
    "# BASELINE MSE OF T + 5\n",
    "b5 = mean_squared_error(data2['demand'].values,\n",
    "                        data2['demand-5'].values)\n",
    "\n",
    "b1, b2, b3, b4, b5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall baseline errors are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T06:08:54.787751Z",
     "start_time": "2019-06-17T06:08:54.780249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001420743482905296"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([b1, b2, b3, b4, b5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model has to beat these baselines for the model to be considered significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST MODEL CONFIGURATIONS\n",
    "\n",
    "Developing a model based on the results of the GA above is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T06:08:57.894681Z",
     "start_time": "2019-06-17T06:08:57.865784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentile          45.000000\n",
       "window              92.000000\n",
       "gen                  1.000000\n",
       "index                9.000000\n",
       "fitness_test_mse     0.001964\n",
       "Name: 19, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all = pd.read_csv('all_records.csv')\n",
    "top = (results_all.sort_values(['fitness_test_mse', 'window'])\n",
    "       .drop_duplicates(['percentile', 'window'])).iloc[0]\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be  observed that the fitness values plateaus within a few generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T06:09:04.861818Z",
     "start_time": "2019-06-17T06:09:04.519447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Fitness (MSE)')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_results_gen = (results_all.groupby('gen').min()['fitness_test_mse']\n",
    "                   .reset_index()['fitness_test_mse'].values)\n",
    "\n",
    "plt.plot(top_results_gen, '-o')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Fitness (MSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T06:48:22.724957Z",
     "start_time": "2019-06-17T06:48:22.715861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best1 = top[['percentile', 'window']].values.astype(int).tolist()\n",
    "\n",
    "train, val, test = create_trainval_test(data=data, pctile=best1[0],\n",
    "                                        max_window=best1[1], frac=0.5)\n",
    "\n",
    "(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    " train_tag, val_tag, test_tag) = split(train, val, test)\n",
    "\n",
    "model = fit_model('model1.hdf5', X_train, y_train,\n",
    "                  X_val, y_val)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:15:29.915818Z",
     "start_time": "2019-06-17T07:15:28.606724Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('model1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:15:53.009057Z",
     "start_time": "2019-06-17T07:15:31.608712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MSE OF T+1 = 0.0006212479622853971\n",
      "TEST MSE OF T+2 = 0.000869136376658637\n",
      "TEST MSE OF T+3 = 0.0010468304397576887\n",
      "TEST MSE OF T+4 = 0.001195712509110794\n",
      "TEST MSE OF T+5 = 0.001311980898222753\n"
     ]
    }
   ],
   "source": [
    "mse1, mse2, mse3, mse4, mse5 = evaluate_indiv_testmse(model, X_test, y_test)\n",
    "\n",
    "print(f'TEST MSE OF T+1 = {mse1}')\n",
    "print(f'TEST MSE OF T+2 = {mse2}')\n",
    "print(f'TEST MSE OF T+3 = {mse3}')\n",
    "print(f'TEST MSE OF T+4 = {mse4}')\n",
    "print(f'TEST MSE OF T+5 = {mse5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:16:17.540241Z",
     "start_time": "2019-06-17T07:15:56.379917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL TEST MSE OF MODEL = 0.001008981637207054\n"
     ]
    }
   ],
   "source": [
    "test_mse = evaluate(model, X_test, y_test)\n",
    "\n",
    "print(f'OVERALL TEST MSE OF MODEL = {test_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen here that the model fitted fits the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTING THE HOLDOUT DATASET\n",
    "\n",
    "This can be seen in the notebook \"Predicting the Holdout Set.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
